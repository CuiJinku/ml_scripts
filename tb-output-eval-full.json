[
    {
        "name": "BERT_pytorch",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 20.516849,
                    "tflops": 1.114356419793795
                },
                {
                    "batch_size": 2,
                    "latency_ms": 10.50857875,
                    "tflops": 3.2352624249520865
                },
                {
                    "batch_size": 4,
                    "latency_ms": 6.002366875,
                    "tflops": 5.642902384985724
                },
                {
                    "batch_size": 8,
                    "latency_ms": 3.0289993749999997,
                    "tflops": 11.133406362173076
                },
                {
                    "batch_size": 16,
                    "latency_ms": 2.6345464375,
                    "tflops": 13.084037945081764
                },
                {
                    "batch_size": 32,
                    "latency_ms": 2.50293428125,
                    "tflops": 14.108262436537398
                },
                {
                    "batch_size": 64,
                    "latency_ms": 2.4293209453125,
                    "tflops": 14.386583001789969
                },
                {
                    "batch_size": 128,
                    "latency_ms": 2.34748198828125,
                    "tflops": 14.886613604069026
                },
                {
                    "batch_size": 256,
                    "latency_ms": 2.2719664941406252,
                    "tflops": 15.392551954025421
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 39.59 GiB total capacity; 37.56 GiB already allocated; 261.94 MiB free; 37.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 256
        }
    },
    {
        "name": "Background_Matting",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "LearningToPaint",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 1436.732078,
                    "tflops": 0.7081587404534341
                },
                {
                    "batch_size": 2,
                    "latency_ms": 729.04542125,
                    "tflops": 1.0495120855796702
                },
                {
                    "batch_size": 4,
                    "latency_ms": 383.132102875,
                    "tflops": 0.8678250164432117
                },
                {
                    "batch_size": 8,
                    "latency_ms": 196.944917375,
                    "tflops": 1.5010650814389381
                },
                {
                    "batch_size": 16,
                    "latency_ms": 106.84969634375,
                    "tflops": 3.1697435630965836
                },
                {
                    "batch_size": 32,
                    "latency_ms": 57.992017515625,
                    "tflops": 4.383182044508083
                },
                {
                    "batch_size": 64,
                    "latency_ms": 35.8756010234375,
                    "tflops": 6.259507318475014
                },
                {
                    "batch_size": 128,
                    "latency_ms": 28.84473085546875,
                    "tflops": 7.2414197547295425
                },
                {
                    "batch_size": 256,
                    "latency_ms": 25.57647844921875,
                    "tflops": 8.118056067965735
                },
                {
                    "batch_size": 512,
                    "latency_ms": 24.05656424316406,
                    "tflops": 8.618082841507663
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 23.32701820458984,
                    "tflops": 8.878633582103708
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 23.075442622558594,
                    "tflops": 8.979099108978469
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 22.808611281494137,
                    "tflops": 9.08510529930661
                }
            ],
            "error_message": "cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.",
            "optimal_tflops_bs": 4096
        }
    },
    {
        "name": "Super_SloMo",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 43.4388665,
                    "tflops": 9.48034146040506
                },
                {
                    "batch_size": 2,
                    "latency_ms": 35.25249275,
                    "tflops": 11.302534644394981
                },
                {
                    "batch_size": 4,
                    "latency_ms": 29.437238875,
                    "tflops": 13.525891603879042
                },
                {
                    "batch_size": 8,
                    "latency_ms": 27.1757161875,
                    "tflops": 14.539758807861496
                },
                {
                    "batch_size": 16,
                    "latency_ms": 16.8228233125,
                    "tflops": 14.703029352315003
                },
                {
                    "batch_size": 32,
                    "latency_ms": 8.4089703125,
                    "tflops": 14.708538026559332
                },
                {
                    "batch_size": 64,
                    "latency_ms": 4.2040293828125,
                    "tflops": 14.728101547917074
                },
                {
                    "batch_size": 128,
                    "latency_ms": 2.1030490234375,
                    "tflops": 14.705642098610857
                },
                {
                    "batch_size": 256,
                    "latency_ms": 1.051676673828125,
                    "tflops": 14.709973293763479
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.5257279394531249,
                    "tflops": 14.66318814030194
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.26286287548828124,
                    "tflops": 14.69913732845438
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.1314540927734375,
                    "tflops": 14.716444455475914
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.06572360766601562,
                    "tflops": 14.692064434823756
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.032865836059570316,
                    "tflops": 14.71123831755928
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.01643108624267578,
                    "tflops": 14.671071779668136
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 0.008217242767333984,
                    "tflops": 14.687511455053988
                },
                {
                    "batch_size": 65536,
                    "latency_ms": 0.0041072674713134765,
                    "tflops": 14.683921440890673
                },
                {
                    "batch_size": 131072,
                    "latency_ms": 0.0020526328544616702,
                    "tflops": 14.684921390236884
                },
                {
                    "batch_size": 262144,
                    "latency_ms": 0.0010268850803375243,
                    "tflops": 14.71646918291485
                },
                {
                    "batch_size": 524288,
                    "latency_ms": 0.00051337064743042,
                    "tflops": 14.698442488292452
                },
                {
                    "batch_size": 1048576,
                    "latency_ms": 0.00025667199516296384,
                    "tflops": 14.705257807759876
                },
                {
                    "batch_size": 2097152,
                    "latency_ms": 0.00012832079482078551,
                    "tflops": 14.729751937472596
                },
                {
                    "batch_size": 4194304,
                    "latency_ms": 6.418542385101318e-05,
                    "tflops": 14.701282208805024
                },
                {
                    "batch_size": 8388608,
                    "latency_ms": 3.208553582429886e-05,
                    "tflops": 14.701162079144357
                },
                {
                    "batch_size": 16777216,
                    "latency_ms": 1.604429864883423e-05,
                    "tflops": 14.70103110008067
                },
                {
                    "batch_size": 33554432,
                    "latency_ms": 8.0226122289896e-06,
                    "tflops": 14.706344475804158
                },
                {
                    "batch_size": 67108864,
                    "latency_ms": 4.012197241187096e-06,
                    "tflops": 14.689157508529442
                },
                {
                    "batch_size": 134217728,
                    "latency_ms": 2.0056308060884477e-06,
                    "tflops": 14.717785436734577
                },
                {
                    "batch_size": 268435456,
                    "latency_ms": 1.0026291534304618e-06,
                    "tflops": 14.699302475534878
                },
                {
                    "batch_size": 536870912,
                    "latency_ms": 5.015739751979709e-07,
                    "tflops": 14.682640229873828
                }
            ],
            "optimal_tflops_bs": 2097152
        }
    },
    {
        "name": "alexnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 1.6256285,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 0.8017427500000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.43456937500000004,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.2318016875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.143893125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.09788634375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.072420390625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.06249595703124999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.055324626953125,
                    "tflops": 12.016610186912036
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.05551198046875,
                    "tflops": 12.759240860258066
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.052988822265625,
                    "tflops": 13.593464915167871
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.05211881201171875,
                    "tflops": 13.815815036124036
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.051207593261718753,
                    "tflops": 13.981196718334305
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.056498088256835936,
                    "tflops": 14.413941855679964
                }
            ],
            "error_message": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
            "optimal_tflops_bs": 8192
        }
    },
    {
        "name": "attention_is_all_you_need_pytorch",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 18.771970500000002,
                    "tflops": 0.2533509213398013
                },
                {
                    "batch_size": 2,
                    "latency_ms": 10.3410405,
                    "tflops": 0.21344931463445094
                },
                {
                    "batch_size": 4,
                    "latency_ms": 5.072121375,
                    "tflops": 0.7236253567343968
                },
                {
                    "batch_size": 8,
                    "latency_ms": 2.9753807500000002,
                    "tflops": 1.035759752163199
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.26042734375,
                    "tflops": 2.6928609789906104
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.7169276875,
                    "tflops": 4.557326263898706
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.315658,
                    "tflops": 10.863701122051014
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.33736675,
                    "tflops": 13.696770512033927
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.31141163867187505,
                    "tflops": 15.286256391155797
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.30404822265625,
                    "tflops": 15.91760718076604
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.3048396904296875,
                    "tflops": 16.2080397190389
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.15246393652343748,
                    "tflops": 16.200822169434066
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.076190283203125,
                    "tflops": 16.209652384667866
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.038099871826171874,
                    "tflops": 16.21155848716357
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.01904846633911133,
                    "tflops": 16.226505408130024
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 0.0095236083984375,
                    "tflops": 16.21113482896168
                },
                {
                    "batch_size": 65536,
                    "latency_ms": 0.004761412811279297,
                    "tflops": 16.20618022901749
                },
                {
                    "batch_size": 131072,
                    "latency_ms": 0.0023813113861083984,
                    "tflops": 16.19373538375064
                },
                {
                    "batch_size": 262144,
                    "latency_ms": 0.001190614833831787,
                    "tflops": 16.203025563091025
                },
                {
                    "batch_size": 524288,
                    "latency_ms": 0.0005952182035446167,
                    "tflops": 16.20491184642273
                },
                {
                    "batch_size": 1048576,
                    "latency_ms": 0.0002976686353683472,
                    "tflops": 16.19696335505791
                },
                {
                    "batch_size": 2097152,
                    "latency_ms": 0.00014883264493942261,
                    "tflops": 16.21412363829371
                },
                {
                    "batch_size": 4194304,
                    "latency_ms": 7.439865946769715e-05,
                    "tflops": 16.224365583789453
                },
                {
                    "batch_size": 8388608,
                    "latency_ms": 3.720184600353241e-05,
                    "tflops": 16.215509248189253
                },
                {
                    "batch_size": 16777216,
                    "latency_ms": 1.8601801782846452e-05,
                    "tflops": 16.20808158823213
                },
                {
                    "batch_size": 33554432,
                    "latency_ms": 9.304227843880653e-06,
                    "tflops": 16.210741470953828
                },
                {
                    "batch_size": 67108864,
                    "latency_ms": 4.653779432177543e-06,
                    "tflops": 16.19557129772866
                },
                {
                    "batch_size": 134217728,
                    "latency_ms": 2.3270045816898346e-06,
                    "tflops": 16.193582786408115
                },
                {
                    "batch_size": 268435456,
                    "latency_ms": 1.162682332098484e-06,
                    "tflops": 16.222817391623494
                },
                {
                    "batch_size": 536870912,
                    "latency_ms": 5.815115217119456e-07,
                    "tflops": 16.204510844003334
                }
            ],
            "optimal_tflops_bs": 16384
        }
    },
    {
        "name": "dcgan",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 1.270156,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 0.643582,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.35173200000000004,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.20269793749999998,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.11624059375000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.064178703125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.0459731015625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.038447734375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.034684013671875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.031615443359375,
                    "tflops": 13.468588901786598
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.030425892578125002,
                    "tflops": 14.36160572460046
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.029643124023437502,
                    "tflops": 14.94490215393634
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.029329715698242186,
                    "tflops": 15.046173957100772
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.029059907775878906,
                    "tflops": 15.209988253725006
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.02426745083618164,
                    "tflops": 15.250146260654127
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 0.023357963150024412,
                    "tflops": 15.598511305649842
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 39.59 GiB total capacity; 27.02 GiB already allocated; 7.11 GiB free; 30.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 32768
        }
    },
    {
        "name": "demucs",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 30.9735275,
                    "tflops": 4.323714704879502
                },
                {
                    "batch_size": 2,
                    "latency_ms": 19.24750325,
                    "tflops": 6.895040003222739
                },
                {
                    "batch_size": 4,
                    "latency_ms": 14.731321125000001,
                    "tflops": 9.426120775946746
                },
                {
                    "batch_size": 8,
                    "latency_ms": 11.80227575,
                    "tflops": 11.598318446189046
                },
                {
                    "batch_size": 16,
                    "latency_ms": 11.672525687499999,
                    "tflops": 12.822537181721891
                },
                {
                    "batch_size": 32,
                    "latency_ms": 9.651182546874999,
                    "tflops": 13.915635250071396
                },
                {
                    "batch_size": 64,
                    "latency_ms": 8.1538642265625,
                    "tflops": 14.88382546886645
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 2.83 GiB (GPU 0; 39.59 GiB total capacity; 35.95 GiB already allocated; 529.94 MiB free; 36.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 64
        }
    },
    {
        "name": "densenet121",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 24.451738499999998,
                    "tflops": 0.2556806894555885
                },
                {
                    "batch_size": 2,
                    "latency_ms": 12.1844945,
                    "tflops": 0.5042176617771347
                },
                {
                    "batch_size": 4,
                    "latency_ms": 6.10821375,
                    "tflops": 0.9649652195338684
                },
                {
                    "batch_size": 8,
                    "latency_ms": 3.1573853124999998,
                    "tflops": 1.9119070716619155
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.70972415625,
                    "tflops": 3.5722808859664403
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.8436155937500001,
                    "tflops": 6.6807131508899875
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.755049296875,
                    "tflops": 8.558701847333811
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.70996569921875,
                    "tflops": 9.18386457052571
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.67638378125,
                    "tflops": 9.642536210922819
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 39.59 GiB total capacity; 36.45 GiB already allocated; 27.94 MiB free; 37.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 256
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_101_c4",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 134.737031,
                    "tflops": 12.381518504645742
                },
                {
                    "batch_size": 2,
                    "latency_ms": 130.885589,
                    "tflops": 12.971628061857388
                },
                {
                    "batch_size": 4,
                    "latency_ms": 118.656409875,
                    "tflops": 13.314290808671261
                },
                {
                    "batch_size": 8,
                    "latency_ms": 118.987906625,
                    "tflops": 13.361375459470263
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 5.00 GiB (GPU 0; 39.59 GiB total capacity; 23.11 GiB already allocated; 3.21 GiB free; 33.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 8
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_101_dc5",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 79.101825,
                    "tflops": 11.77746011003922
                },
                {
                    "batch_size": 2,
                    "latency_ms": 76.98907,
                    "tflops": 12.137367287976351
                },
                {
                    "batch_size": 4,
                    "latency_ms": 83.95261975,
                    "tflops": 13.62690708927239
                },
                {
                    "batch_size": 8,
                    "latency_ms": 83.049676,
                    "tflops": 13.894351658019753
                },
                {
                    "batch_size": 16,
                    "latency_ms": 89.88828321875,
                    "tflops": 14.217158268971623
                },
                {
                    "batch_size": 32,
                    "latency_ms": 71.66626446875,
                    "tflops": 14.280330277403525
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.81 GiB (GPU 0; 39.59 GiB total capacity; 19.97 GiB already allocated; 4.36 GiB free; 32.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_101_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 51.5630605,
                    "tflops": 9.689082237229501
                },
                {
                    "batch_size": 2,
                    "latency_ms": 45.96151425,
                    "tflops": 10.819751694845097
                },
                {
                    "batch_size": 4,
                    "latency_ms": 66.84438962499999,
                    "tflops": 12.717973729568138
                },
                {
                    "batch_size": 8,
                    "latency_ms": 56.46501775,
                    "tflops": 12.575254704611659
                },
                {
                    "batch_size": 16,
                    "latency_ms": 55.26244096875,
                    "tflops": 12.613603168551204
                },
                {
                    "batch_size": 32,
                    "latency_ms": 53.853478,
                    "tflops": 12.511348164367423
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.86 GiB already allocated; 6.09 GiB free; 31.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_50_c4",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 122.135843,
                    "tflops": 12.651333337222056
                },
                {
                    "batch_size": 2,
                    "latency_ms": 119.6167055,
                    "tflops": 13.020134208318028
                },
                {
                    "batch_size": 4,
                    "latency_ms": 103.505941,
                    "tflops": 13.206247860949457
                },
                {
                    "batch_size": 8,
                    "latency_ms": 95.5551531875,
                    "tflops": 13.23242824267645
                },
                {
                    "batch_size": 16,
                    "latency_ms": 87.24982928125,
                    "tflops": 13.16338033268922
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 8.70 GiB (GPU 0; 39.59 GiB total capacity; 30.74 GiB already allocated; 3.39 GiB free; 33.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 8
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_50_dc5",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 65.69673,
                    "tflops": 11.812651592074232
                },
                {
                    "batch_size": 2,
                    "latency_ms": 65.5241495,
                    "tflops": 12.099828540816912
                },
                {
                    "batch_size": 4,
                    "latency_ms": 69.89155650000001,
                    "tflops": 13.569017385564287
                },
                {
                    "batch_size": 8,
                    "latency_ms": 65.9895758125,
                    "tflops": 13.96396899553778
                },
                {
                    "batch_size": 16,
                    "latency_ms": 71.5670646875,
                    "tflops": 13.920835560018407
                },
                {
                    "batch_size": 32,
                    "latency_ms": 55.94929090625,
                    "tflops": 14.305261855537417
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.81 GiB (GPU 0; 39.59 GiB total capacity; 19.91 GiB already allocated; 6.12 GiB free; 31.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_50_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 38.270736,
                    "tflops": 8.982485732179645
                },
                {
                    "batch_size": 2,
                    "latency_ms": 34.25472725,
                    "tflops": 10.071329644676691
                },
                {
                    "batch_size": 4,
                    "latency_ms": 51.751765125,
                    "tflops": 12.40620414023485
                },
                {
                    "batch_size": 8,
                    "latency_ms": 41.759209874999996,
                    "tflops": 11.944575403209978
                },
                {
                    "batch_size": 16,
                    "latency_ms": 38.960852687499994,
                    "tflops": 11.589561679674308
                },
                {
                    "batch_size": 32,
                    "latency_ms": 40.168725671874995,
                    "tflops": 12.05214234014183
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.79 GiB already allocated; 6.34 GiB free; 30.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "detectron2_fcos_r_50_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 40.740554,
                    "tflops": 8.220398680475983
                },
                {
                    "batch_size": 2,
                    "latency_ms": 37.65934925,
                    "tflops": 9.218820708240163
                },
                {
                    "batch_size": 4,
                    "latency_ms": 43.310626625,
                    "tflops": 11.022564394125881
                },
                {
                    "batch_size": 8,
                    "latency_ms": 42.3042825,
                    "tflops": 11.288855717889987
                },
                {
                    "batch_size": 16,
                    "latency_ms": 40.876764593749996,
                    "tflops": 10.696403876555761
                },
                {
                    "batch_size": 32,
                    "latency_ms": 37.586720390625004,
                    "tflops": 9.973733439933403
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 19.64 GiB already allocated; 6.76 GiB free; 30.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 8
        }
    },
    {
        "name": "detectron2_maskrcnn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 4038.2976525000004,
                    "tflops": 8.868408164760673
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2048.28513225,
                    "tflops": 10.344351101880031
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1162.8159056250001,
                    "tflops": 11.679526806498423
                },
                {
                    "batch_size": 8,
                    "latency_ms": 577.934581625,
                    "tflops": 11.71808996975303
                },
                {
                    "batch_size": 16,
                    "latency_ms": 263.43622528125,
                    "tflops": 11.320918289521863
                },
                {
                    "batch_size": 32,
                    "latency_ms": 139.439270984375,
                    "tflops": 11.821883449355894
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.80 GiB already allocated; 6.22 GiB free; 30.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "detectron2_maskrcnn_r_101_c4",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 144.916114,
                    "tflops": 12.332690892830625
                },
                {
                    "batch_size": 2,
                    "latency_ms": 141.6159445,
                    "tflops": 12.74172168417993
                },
                {
                    "batch_size": 4,
                    "latency_ms": 126.05320037499999,
                    "tflops": 13.193299412479954
                },
                {
                    "batch_size": 8,
                    "latency_ms": 126.1092716875,
                    "tflops": 13.300217329000391
                },
                {
                    "batch_size": 16,
                    "latency_ms": 113.49558987500001,
                    "tflops": 14.132633912060314
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 10.32 GiB (GPU 0; 39.59 GiB total capacity; 25.76 GiB already allocated; 7.55 GiB free; 29.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 16
        }
    },
    {
        "name": "detectron2_maskrcnn_r_101_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 55.600428,
                    "tflops": 9.323823011984308
                },
                {
                    "batch_size": 2,
                    "latency_ms": 50.10794525,
                    "tflops": 10.526159246297546
                },
                {
                    "batch_size": 4,
                    "latency_ms": 69.576135625,
                    "tflops": 12.482485429046324
                },
                {
                    "batch_size": 8,
                    "latency_ms": 59.039953062500004,
                    "tflops": 12.264467884369083
                },
                {
                    "batch_size": 16,
                    "latency_ms": 57.35273,
                    "tflops": 12.42021073880411
                },
                {
                    "batch_size": 32,
                    "latency_ms": 56.252735578125,
                    "tflops": 12.379351425019536
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.87 GiB already allocated; 6.00 GiB free; 31.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "detectron2_maskrcnn_r_50_c4",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 133.1222075,
                    "tflops": 12.410522088265262
                },
                {
                    "batch_size": 2,
                    "latency_ms": 128.84541025,
                    "tflops": 12.873398145359625
                },
                {
                    "batch_size": 4,
                    "latency_ms": 111.679856875,
                    "tflops": 13.107094978381866
                },
                {
                    "batch_size": 8,
                    "latency_ms": 102.509980875,
                    "tflops": 13.08328412971262
                },
                {
                    "batch_size": 16,
                    "latency_ms": 92.54606506249999,
                    "tflops": 13.013984822873235
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 8.63 GiB (GPU 0; 39.59 GiB total capacity; 30.51 GiB already allocated; 3.60 GiB free; 33.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "detectron2_maskrcnn_r_50_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 42.676658,
                    "tflops": 8.62441634086373
                },
                {
                    "batch_size": 2,
                    "latency_ms": 38.025744,
                    "tflops": 9.66359611990479
                },
                {
                    "batch_size": 4,
                    "latency_ms": 55.3033475,
                    "tflops": 11.976671345448517
                },
                {
                    "batch_size": 8,
                    "latency_ms": 44.946095125,
                    "tflops": 11.645229813423777
                },
                {
                    "batch_size": 16,
                    "latency_ms": 41.30584159375,
                    "tflops": 11.29501412120724
                },
                {
                    "batch_size": 32,
                    "latency_ms": 42.884091359375,
                    "tflops": 11.750423838222574
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.80 GiB already allocated; 6.18 GiB free; 30.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "dlrm",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 2.1851564999999997,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 1.12402675,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.5342709999999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.263863125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.1391480625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.06642803124999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.0355271171875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.018113335937500002,
                    "tflops": 0.0
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.008237771484375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.0045426767578125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.0020589726562499997,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.001140303466796875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.000550489013671875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.00026933099365234375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.00013210845947265624,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 7.005654907226564e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 65536,
                    "latency_ms": 3.377298736572266e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 131072,
                    "latency_ms": 1.606056213378906e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 262144,
                    "latency_ms": 8.0443115234375e-06,
                    "tflops": 0.0
                },
                {
                    "batch_size": 524288,
                    "latency_ms": 4.140338897705078e-06,
                    "tflops": 0.0
                },
                {
                    "batch_size": 1048576,
                    "latency_ms": 2.0260958671569826e-06,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2097152,
                    "latency_ms": 1.063246488571167e-06,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4194304,
                    "latency_ms": 5.050581693649291e-07,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8388608,
                    "latency_ms": 2.6261967420578e-07,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16777216,
                    "latency_ms": 1.330336034297943e-07,
                    "tflops": 0.0
                },
                {
                    "batch_size": 33554432,
                    "latency_ms": 6.721332669258117e-08,
                    "tflops": 0.0
                },
                {
                    "batch_size": 67108864,
                    "latency_ms": 3.244417905807495e-08,
                    "tflops": 0.0
                },
                {
                    "batch_size": 134217728,
                    "latency_ms": 1.6145259141921995e-08,
                    "tflops": 0.0
                },
                {
                    "batch_size": 268435456,
                    "latency_ms": 7.802469655871392e-09,
                    "tflops": 0.0
                },
                {
                    "batch_size": 536870912,
                    "latency_ms": 3.995492123067379e-09,
                    "tflops": 0.0
                }
            ],
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "drq",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "fambench_dlrm",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "TypeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "\n'numpy.int64' object in attribute 'EmbeddingBag.num_embeddings' is not a valid constant.\nValid constants are:\n1. a nn.ModuleList\n2. a value of type {bool, float, int, str, NoneType, torch.device, torch.layout, torch.dtype}\n3. a list or tuple of (2)\n"
        }
    },
    {
        "name": "fambench_xlmr",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 21.1463745,
                    "tflops": 2.412054853283105
                },
                {
                    "batch_size": 2,
                    "latency_ms": 15.4375275,
                    "tflops": 3.3888658861557395
                },
                {
                    "batch_size": 4,
                    "latency_ms": 5.4468235,
                    "tflops": 9.388023252200046
                },
                {
                    "batch_size": 8,
                    "latency_ms": 4.0437574375,
                    "tflops": 12.817737011665455
                },
                {
                    "batch_size": 16,
                    "latency_ms": 3.4117111875,
                    "tflops": 15.341016169228348
                },
                {
                    "batch_size": 32,
                    "latency_ms": 3.26090928125,
                    "tflops": 16.199341277284624
                },
                {
                    "batch_size": 64,
                    "latency_ms": 3.1486744296875,
                    "tflops": 16.661506548922937
                },
                {
                    "batch_size": 128,
                    "latency_ms": 3.0964266796875,
                    "tflops": 16.94440485306363
                },
                {
                    "batch_size": 256,
                    "latency_ms": 2.954384095703125,
                    "tflops": 17.779684262137817
                },
                {
                    "batch_size": 512,
                    "latency_ms": 2.9519376396484374,
                    "tflops": 17.78769488350485
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 2.9210103374023437,
                    "tflops": 17.984463838555826
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 2.9147430744628906,
                    "tflops": 18.016057119667643
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 2.91013179296875,
                    "tflops": 18.041376028711902
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 2.908889839050293,
                    "tflops": 18.05290088474568
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.59 GiB total capacity; 26.09 GiB already allocated; 9.81 GiB free; 28.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 8192
        }
    },
    {
        "name": "fastNLP_Bert",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 66.59118649999999,
                    "tflops": 7.613032827838703
                },
                {
                    "batch_size": 2,
                    "latency_ms": 46.30519775,
                    "tflops": 10.878415543343039
                },
                {
                    "batch_size": 4,
                    "latency_ms": 43.060996625,
                    "tflops": 11.745179761572727
                },
                {
                    "batch_size": 8,
                    "latency_ms": 39.2979473125,
                    "tflops": 12.815637621503148
                },
                {
                    "batch_size": 16,
                    "latency_ms": 37.57878425,
                    "tflops": 13.311149842060411
                },
                {
                    "batch_size": 32,
                    "latency_ms": 36.555930328125,
                    "tflops": 13.709779431727979
                },
                {
                    "batch_size": 64,
                    "latency_ms": 35.5519908515625,
                    "tflops": 14.073026031636534
                },
                {
                    "batch_size": 128,
                    "latency_ms": 34.81877467578125,
                    "tflops": 14.357944173925409
                },
                {
                    "batch_size": 256,
                    "latency_ms": 34.67431259375,
                    "tflops": 14.413635097489536
                },
                {
                    "batch_size": 512,
                    "latency_ms": 34.62998261328125,
                    "tflops": 14.45039542310983
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 39.59 GiB total capacity; 21.30 GiB already allocated; 2.09 GiB free; 35.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 512
        }
    },
    {
        "name": "hf_Albert",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 14.3090935,
                    "tflops": 9.119662247857475
                },
                {
                    "batch_size": 2,
                    "latency_ms": 11.81190325,
                    "tflops": 11.4189963387329
                },
                {
                    "batch_size": 4,
                    "latency_ms": 11.0874915,
                    "tflops": 12.433560764822634
                },
                {
                    "batch_size": 8,
                    "latency_ms": 10.564710187500001,
                    "tflops": 13.25281036323677
                },
                {
                    "batch_size": 16,
                    "latency_ms": 10.20679421875,
                    "tflops": 13.64712617118564
                },
                {
                    "batch_size": 32,
                    "latency_ms": 9.84728546875,
                    "tflops": 14.1760066382842
                },
                {
                    "batch_size": 64,
                    "latency_ms": 9.564292304687498,
                    "tflops": 14.60322266436817
                },
                {
                    "batch_size": 128,
                    "latency_ms": 9.4226437890625,
                    "tflops": 14.835465156919097
                },
                {
                    "batch_size": 256,
                    "latency_ms": 9.3925429921875,
                    "tflops": 14.8803397319494
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 29.30 GiB (GPU 0; 39.59 GiB total capacity; 941.57 MiB already allocated; 8.41 GiB free; 29.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 256
        }
    },
    {
        "name": "hf_Bart",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 20.0610595,
                    "tflops": 10.26054223758452
                },
                {
                    "batch_size": 2,
                    "latency_ms": 16.16847675,
                    "tflops": 12.999659559957852
                },
                {
                    "batch_size": 4,
                    "latency_ms": 14.692930624999999,
                    "tflops": 14.41005899840772
                },
                {
                    "batch_size": 8,
                    "latency_ms": 14.006328562499998,
                    "tflops": 15.394575636413949
                },
                {
                    "batch_size": 16,
                    "latency_ms": 13.675375375,
                    "tflops": 15.574583623080727
                },
                {
                    "batch_size": 32,
                    "latency_ms": 13.241152578125,
                    "tflops": 16.163871143021996
                },
                {
                    "batch_size": 64,
                    "latency_ms": 12.88913734375,
                    "tflops": 16.620571581661427
                },
                {
                    "batch_size": 128,
                    "latency_ms": 12.812043078125,
                    "tflops": 16.74328369893548
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 24.54 GiB (GPU 0; 39.59 GiB total capacity; 10.27 GiB already allocated; 24.49 GiB free; 13.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "hf_Bert",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 15.146292500000001,
                    "tflops": 10.012419639634098
                },
                {
                    "batch_size": 2,
                    "latency_ms": 12.6569235,
                    "tflops": 12.645056852171761
                },
                {
                    "batch_size": 4,
                    "latency_ms": 11.589072125,
                    "tflops": 14.010316910493982
                },
                {
                    "batch_size": 8,
                    "latency_ms": 11.06549575,
                    "tflops": 15.084195706487078
                },
                {
                    "batch_size": 16,
                    "latency_ms": 10.6960581875,
                    "tflops": 15.472397422105919
                },
                {
                    "batch_size": 32,
                    "latency_ms": 10.36651009375,
                    "tflops": 15.991536622340343
                },
                {
                    "batch_size": 64,
                    "latency_ms": 10.085490390625,
                    "tflops": 16.43597087956737
                },
                {
                    "batch_size": 128,
                    "latency_ms": 9.95506546484375,
                    "tflops": 16.678158164110876
                },
                {
                    "batch_size": 256,
                    "latency_ms": 9.919696953125,
                    "tflops": 16.768959229480842
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 29.81 GiB (GPU 0; 39.59 GiB total capacity; 1.91 GiB already allocated; 6.84 GiB free; 31.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 256
        }
    },
    {
        "name": "hf_BigBird",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 164.3489485,
                    "tflops": 10.361886265253004
                },
                {
                    "batch_size": 2,
                    "latency_ms": 144.1981085,
                    "tflops": 11.764890489662204
                },
                {
                    "batch_size": 4,
                    "latency_ms": 132.27642975,
                    "tflops": 12.879766785156326
                },
                {
                    "batch_size": 8,
                    "latency_ms": 125.288194875,
                    "tflops": 13.671231442166397
                },
                {
                    "batch_size": 16,
                    "latency_ms": 123.46945565625,
                    "tflops": 13.861390621020725
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 24.59 GiB (GPU 0; 39.59 GiB total capacity; 1.23 GiB already allocated; 12.73 GiB free; 25.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 16
        }
    },
    {
        "name": "hf_DistilBert",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 8.6691185,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 7.29704175,
                    "tflops": 12.491611258462965
                },
                {
                    "batch_size": 4,
                    "latency_ms": 6.75323675,
                    "tflops": 14.0126932889928
                },
                {
                    "batch_size": 8,
                    "latency_ms": 6.49468525,
                    "tflops": 15.112088781939159
                },
                {
                    "batch_size": 16,
                    "latency_ms": 6.273878375,
                    "tflops": 15.688036441337406
                },
                {
                    "batch_size": 32,
                    "latency_ms": 6.085484015625,
                    "tflops": 16.14028388929431
                },
                {
                    "batch_size": 64,
                    "latency_ms": 5.937348265625,
                    "tflops": 16.576918229754657
                },
                {
                    "batch_size": 128,
                    "latency_ms": 5.8692083203125005,
                    "tflops": 16.859192251012765
                },
                {
                    "batch_size": 256,
                    "latency_ms": 5.84890024609375,
                    "tflops": 16.922062415121143
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 29.81 GiB (GPU 0; 39.59 GiB total capacity; 1.75 GiB already allocated; 7.02 GiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 256
        }
    },
    {
        "name": "hf_GPT2",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 30.9856135,
                    "tflops": 12.885291243733423
                },
                {
                    "batch_size": 2,
                    "latency_ms": 29.6573865,
                    "tflops": 13.606366252565081
                },
                {
                    "batch_size": 4,
                    "latency_ms": 29.05106975,
                    "tflops": 14.014155969704492
                },
                {
                    "batch_size": 8,
                    "latency_ms": 28.167493937499998,
                    "tflops": 14.444699297023748
                },
                {
                    "batch_size": 16,
                    "latency_ms": 27.428823843750003,
                    "tflops": 14.846401674352277
                },
                {
                    "batch_size": 32,
                    "latency_ms": 27.112639156249998,
                    "tflops": 15.084833690930246
                },
                {
                    "batch_size": 64,
                    "latency_ms": 26.89465034375,
                    "tflops": 15.232897486523573
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 24.54 GiB (GPU 0; 39.59 GiB total capacity; 14.35 GiB already allocated; 17.84 GiB free; 20.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 64
        }
    },
    {
        "name": "hf_Longformer",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 121.6403235,
                    "tflops": 12.961703201455242
                },
                {
                    "batch_size": 2,
                    "latency_ms": 121.34352225,
                    "tflops": 13.061580872996455
                },
                {
                    "batch_size": 4,
                    "latency_ms": 117.827676375,
                    "tflops": 13.466752666360653
                },
                {
                    "batch_size": 8,
                    "latency_ms": 115.17281993750001,
                    "tflops": 13.815710614643347
                },
                {
                    "batch_size": 16,
                    "latency_ms": 113.99267146874999,
                    "tflops": 13.954352119722493
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 24.54 GiB (GPU 0; 39.59 GiB total capacity; 1.31 GiB already allocated; 12.23 GiB free; 25.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 16
        }
    },
    {
        "name": "hf_Reformer",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 16.1129285,
                    "tflops": 6.012956145246293
                },
                {
                    "batch_size": 2,
                    "latency_ms": 12.09614375,
                    "tflops": 8.431463031082213
                },
                {
                    "batch_size": 4,
                    "latency_ms": 10.845086125,
                    "tflops": 9.588339308624429
                },
                {
                    "batch_size": 8,
                    "latency_ms": 10.087245625,
                    "tflops": 10.432552418238217
                },
                {
                    "batch_size": 16,
                    "latency_ms": 9.747177656249999,
                    "tflops": 10.923581058409262
                },
                {
                    "batch_size": 32,
                    "latency_ms": 9.645283296875,
                    "tflops": 11.0338263209179
                },
                {
                    "batch_size": 64,
                    "latency_ms": 9.5697581796875,
                    "tflops": 11.142353837758206
                },
                {
                    "batch_size": 128,
                    "latency_ms": 9.578439050781249,
                    "tflops": 11.105104316093504
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.00 GiB (GPU 0; 39.59 GiB total capacity; 27.41 GiB already allocated; 2.86 GiB free; 35.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 64
        }
    },
    {
        "name": "hf_T5",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 70.52876499999999,
                    "tflops": 9.178204389419196
                },
                {
                    "batch_size": 2,
                    "latency_ms": 61.21992625,
                    "tflops": 10.610826256107398
                },
                {
                    "batch_size": 4,
                    "latency_ms": 56.298687125,
                    "tflops": 11.58412006826262
                },
                {
                    "batch_size": 8,
                    "latency_ms": 55.450570937500004,
                    "tflops": 11.784016321761204
                },
                {
                    "batch_size": 16,
                    "latency_ms": 53.736486375,
                    "tflops": 12.140350823126429
                },
                {
                    "batch_size": 32,
                    "latency_ms": 53.478466624999996,
                    "tflops": 12.224702187153882
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 39.59 GiB total capacity; 27.48 GiB already allocated; 3.67 GiB free; 34.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "maml",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "maml_omniglot",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "mnasnet1_0",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 6.899935,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 3.761815,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.0867728750000003,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.9684619375000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.488084625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.267563609375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.2266598828125,
                    "tflops": 4.152972463648671
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.21202373046875,
                    "tflops": 4.727223009175927
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.200580029296875,
                    "tflops": 4.896024616816675
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.1949122119140625,
                    "tflops": 5.111985778581676
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 39.59 GiB total capacity; 36.83 GiB already allocated; 239.94 MiB free; 36.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 512
        }
    },
    {
        "name": "mobilenet_v2",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 8.092877,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 4.1274505,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.151635625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.0716508125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.5761541875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.283220953125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.2591861640625,
                    "tflops": 3.3107677522766563
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.24447391015625,
                    "tflops": 3.9332033628348584
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.23339702148437502,
                    "tflops": 4.046339533032677
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 39.59 GiB total capacity; 37.07 GiB already allocated; 17.94 MiB free; 37.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 256
        }
    },
    {
        "name": "mobilenet_v2_quantized_qat",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "NotImplemented",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "The eval test only supports CPU."
        }
    },
    {
        "name": "mobilenet_v3_large",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 9.5173545,
                    "tflops": 0.08758715622871208
                },
                {
                    "batch_size": 2,
                    "latency_ms": 4.819568,
                    "tflops": 0.1568829530231085
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.59842175,
                    "tflops": 0.2606983141717941
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.289517,
                    "tflops": 0.5415006854819278
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.63044465625,
                    "tflops": 1.088861164210512
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.3344484375,
                    "tflops": 1.9863868720403532
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.1992807734375,
                    "tflops": 3.251898199363619
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.1825265546875,
                    "tflops": 4.050807884231468
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.17015530078125002,
                    "tflops": 4.362292164264952
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.16340108691406252,
                    "tflops": 4.538023235085792
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 516.00 MiB (GPU 0; 39.59 GiB total capacity; 37.00 GiB already allocated; 99.94 MiB free; 37.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 512
        }
    },
    {
        "name": "moco",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 67.2347085,
                    "tflops": 0.6547518775341121
                },
                {
                    "batch_size": 2,
                    "latency_ms": 33.08965325,
                    "tflops": 1.2753631055336447
                },
                {
                    "batch_size": 4,
                    "latency_ms": 16.8030775,
                    "tflops": 2.2776803663023353
                },
                {
                    "batch_size": 8,
                    "latency_ms": 10.2923696875,
                    "tflops": 3.7147665023800647
                },
                {
                    "batch_size": 16,
                    "latency_ms": 5.02501140625,
                    "tflops": 6.9148371838732965
                },
                {
                    "batch_size": 32,
                    "latency_ms": 3.831750109375,
                    "tflops": 8.694908282663773
                },
                {
                    "batch_size": 64,
                    "latency_ms": 3.309715078125,
                    "tflops": 10.076422074283835
                },
                {
                    "batch_size": 128,
                    "latency_ms": 3.04574341796875,
                    "tflops": 10.837154341466324
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 39.59 GiB total capacity; 36.18 GiB already allocated; 187.94 MiB free; 36.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "nvidia_deeprecommender",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 1.1389925,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 0.67321975,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.343043125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.22249212499999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.10167109375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.053842078125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.043330726562500005,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.0383608359375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.034023814453124995,
                    "tflops": 0.0
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.031491259765625,
                    "tflops": 15.985628300378874
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.030879340332031248,
                    "tflops": 17.06263882287068
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.030402323974609374,
                    "tflops": 17.50087406864231
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.030143860717773436,
                    "tflops": 17.764422366413967
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.02988779833984375,
                    "tflops": 18.020476097053628
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.031482424804687506,
                    "tflops": 17.083307929591466
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 388.00 MiB (GPU 0; 39.59 GiB total capacity; 37.39 GiB already allocated; 147.94 MiB free; 37.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 8192
        }
    },
    {
        "name": "opacus_cifar10",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 5.4413944999999995,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.568313,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.29625875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.684781125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.35803018750000004,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.20967528125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.134016109375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.10541105078124999,
                    "tflops": 9.86616976605275
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.085767181640625,
                    "tflops": 13.320714864353123
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.0731080439453125,
                    "tflops": 15.687672937663692
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.0280094296875,
                    "tflops": 13.849563488024167
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.07184661694335936,
                    "tflops": 17.13186339835836
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.02588280517578125,
                    "tflops": 15.405749596072459
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.0704394556274414,
                    "tflops": 17.510817644063323
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.037313411865234375,
                    "tflops": 16.51540668611096
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 0.03717467152404785,
                    "tflops": 16.578553985317537
                },
                {
                    "batch_size": 65536,
                    "latency_ms": 0.02181816418457031,
                    "tflops": 15.881561900273889
                },
                {
                    "batch_size": 131072,
                    "latency_ms": 0.02179823876953125,
                    "tflops": 15.905051679797808
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.59 GiB total capacity; 19.09 GiB already allocated; 11.03 GiB free; 26.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 8192
        }
    },
    {
        "name": "pyhpc_equation_of_state",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 4.453571999999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.29818925,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.154724625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.5855696875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.27732403125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.138792359375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.06647635937499999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.03343859765625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.016776171875000002,
                    "tflops": 0.0
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.008816219726562501,
                    "tflops": 0.0
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.00458301611328125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.002089126708984375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.0010320496826171875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.0005190113525390625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.00028174462890625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 0.0001392583770751953,
                    "tflops": 0.0
                },
                {
                    "batch_size": 65536,
                    "latency_ms": 7.011489868164063e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 131072,
                    "latency_ms": 3.445661926269531e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 262144,
                    "latency_ms": 1.8083585739135742e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 524288,
                    "latency_ms": 9.010619163513185e-06,
                    "tflops": 0.0
                },
                {
                    "batch_size": 1048576,
                    "latency_ms": 7.1400399208068855e-06,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2097152,
                    "latency_ms": 6.521305322647094e-06,
                    "tflops": 0.0024644512977178906
                },
                {
                    "batch_size": 4194304,
                    "latency_ms": 6.493068456649781e-06,
                    "tflops": 0.002649621268140094
                },
                {
                    "batch_size": 8388608,
                    "latency_ms": 5.976920366287231e-06,
                    "tflops": 0.0027765237280619353
                },
                {
                    "batch_size": 16777216,
                    "latency_ms": 5.756052523851395e-06,
                    "tflops": 0.0030408951790829386
                },
                {
                    "batch_size": 33554432,
                    "latency_ms": 5.694848135113717e-06,
                    "tflops": 0.0029673556039816053
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 516.00 MiB (GPU 0; 39.59 GiB total capacity; 37.67 GiB already allocated; 129.94 MiB free; 37.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 16777216
        }
    },
    {
        "name": "pyhpc_isoneutral_mixing",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 5.125603999999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.98504475,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.3168233749999998,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.6482898125000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.39036375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.229480875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.0973056953125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.06678317187499999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.039329859375,
                    "tflops": 1.4323220061130855e-06
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.017736792968749998,
                    "tflops": 0.0
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.008420216796875001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.004184989501953125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.0023672391357421876,
                    "tflops": 2.6759974187988643e-05
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.0010841168823242189,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.0005165589904785157,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 0.00026178056335449216,
                    "tflops": 0.0
                },
                {
                    "batch_size": 65536,
                    "latency_ms": 0.00012989959716796874,
                    "tflops": 0.0
                },
                {
                    "batch_size": 131072,
                    "latency_ms": 6.613554763793946e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 262144,
                    "latency_ms": 3.3447771072387696e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 524288,
                    "latency_ms": 1.6616646766662597e-05,
                    "tflops": 0.0
                },
                {
                    "batch_size": 1048576,
                    "latency_ms": 9.792386531829834e-06,
                    "tflops": 0.009723389223437677
                },
                {
                    "batch_size": 2097152,
                    "latency_ms": 8.17249083518982e-06,
                    "tflops": 0.011589177119812995
                },
                {
                    "batch_size": 4194304,
                    "latency_ms": 8.300159454345703e-06,
                    "tflops": 0.012480001440625733
                },
                {
                    "batch_size": 8388608,
                    "latency_ms": 7.679677963256836e-06,
                    "tflops": 0.01365343058852776
                },
                {
                    "batch_size": 16777216,
                    "latency_ms": 7.3580900430679315e-06,
                    "tflops": 0.014080854381292415
                },
                {
                    "batch_size": 33554432,
                    "latency_ms": 7.321073025465012e-06,
                    "tflops": 0.014552342231572155
                },
                {
                    "batch_size": 67108864,
                    "latency_ms": 7.247482821345329e-06,
                    "tflops": 0.014721056206544552
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 39.59 GiB total capacity; 37.07 GiB already allocated; 821.94 MiB free; 37.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 67108864
        }
    },
    {
        "name": "pyhpc_turbulent_kinetic_energy",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "IndexError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "index -2 is out of bounds for dimension 2 with size 1"
        }
    },
    {
        "name": "pytorch_CycleGAN_and_pix2pix",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "pytorch_stargan",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "NotImplemented",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model doesn't support customizing batch size."
        }
    },
    {
        "name": "pytorch_struct",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "max() arg is an empty sequence"
        }
    },
    {
        "name": "pytorch_unet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 69.2739505,
                    "tflops": 13.894694794635784
                },
                {
                    "batch_size": 2,
                    "latency_ms": 64.77162799999999,
                    "tflops": 14.889715579166708
                },
                {
                    "batch_size": 4,
                    "latency_ms": 61.657775375,
                    "tflops": 15.750149524745346
                },
                {
                    "batch_size": 8,
                    "latency_ms": 60.5831825,
                    "tflops": 16.039695378956186
                },
                {
                    "batch_size": 16,
                    "latency_ms": 59.9525455625,
                    "tflops": 16.24434320943401
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 4.68 GiB (GPU 0; 39.59 GiB total capacity; 29.10 GiB already allocated; 3.70 GiB free; 33.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 16
        }
    },
    {
        "name": "resnet18",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 4.300504,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.2141275,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.093667,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.5970578125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.3256706875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.24283296875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.2071067421875,
                    "tflops": 9.46761382286833
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.19543480859375,
                    "tflops": 10.332245876796124
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.18458421679687498,
                    "tflops": 10.80867550707004
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.1831814365234375,
                    "tflops": 11.30352455829927
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.17382523828125002,
                    "tflops": 11.532590550943587
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 39.59 GiB total capacity; 34.11 GiB already allocated; 99.94 MiB free; 37.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 1024
        }
    },
    {
        "name": "resnet50",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 9.2852165,
                    "tflops": 1.030413342946511
                },
                {
                    "batch_size": 2,
                    "latency_ms": 5.9325985,
                    "tflops": 1.648035072246048
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.52386175,
                    "tflops": 3.3190770593893943
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.4147621875,
                    "tflops": 5.949218835026933
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.9171419062499999,
                    "tflops": 8.080965814637565
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.77607665625,
                    "tflops": 10.144059480438285
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.7170991718749999,
                    "tflops": 11.207704801611756
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.68535386328125,
                    "tflops": 11.818081296792213
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.660041552734375,
                    "tflops": 12.071697006812103
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 39.59 GiB total capacity; 36.18 GiB already allocated; 249.94 MiB free; 36.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 256
        }
    },
    {
        "name": "resnet50_quantized_qat",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "NotImplemented",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "The eval test only supports CPU."
        }
    },
    {
        "name": "resnext50_32x4d",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 10.144838499999999,
                    "tflops": 1.0931854382199242
                },
                {
                    "batch_size": 2,
                    "latency_ms": 5.27185175,
                    "tflops": 2.281807231291328
                },
                {
                    "batch_size": 4,
                    "latency_ms": 3.1783117499999998,
                    "tflops": 3.5916161878397648
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.6682266875,
                    "tflops": 6.49768365606624
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.31405740625,
                    "tflops": 9.197268313391394
                },
                {
                    "batch_size": 32,
                    "latency_ms": 1.1166138125,
                    "tflops": 11.098306387322454
                },
                {
                    "batch_size": 64,
                    "latency_ms": 1.0351308046874999,
                    "tflops": 12.187204921618257
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.9834608828125,
                    "tflops": 12.828139193198721
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.9532392968750001,
                    "tflops": 13.267116385042364
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 39.59 GiB total capacity; 36.75 GiB already allocated; 227.94 MiB free; 36.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 256
        }
    },
    {
        "name": "shufflenet_v2_x1_0",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 8.5539855,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 4.64712575,
                    "tflops": 0.09447742867969547
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.329612875,
                    "tflops": 0.1652235634996274
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.1350251875,
                    "tflops": 0.36480828716127695
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.5677614375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.312642140625,
                    "tflops": 1.263258005663804
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.1654645234375,
                    "tflops": 2.570981558787689
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.09752399609375,
                    "tflops": 4.341778936001534
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.091961072265625,
                    "tflops": 4.725966327293737
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.087694091796875,
                    "tflops": 5.065096248425284
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.08497641064453125,
                    "tflops": 5.333540059396215
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 39.59 GiB total capacity; 37.12 GiB already allocated; 9.94 MiB free; 37.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 1024
        }
    },
    {
        "name": "soft_actor_critic",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "NotImplemented",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model doesn't support customizing batch size."
        }
    },
    {
        "name": "speech_transformer",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 5756.7600145,
                    "tflops": 0.49822108678479093
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2896.82532125,
                    "tflops": 0.49466552445156
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1452.0638410000001,
                    "tflops": 0.49413168948198477
                },
                {
                    "batch_size": 8,
                    "latency_ms": 726.436155,
                    "tflops": 0.49255800283631257
                },
                {
                    "batch_size": 16,
                    "latency_ms": 365.33599846874995,
                    "tflops": 0.4845814591522869
                },
                {
                    "batch_size": 32,
                    "latency_ms": 180.128492,
                    "tflops": 0.4967204450034963
                },
                {
                    "batch_size": 64,
                    "latency_ms": 90.34492571875,
                    "tflops": 0.4949161135645341
                },
                {
                    "batch_size": 128,
                    "latency_ms": 45.2744046328125,
                    "tflops": 0.49539537275862255
                },
                {
                    "batch_size": 256,
                    "latency_ms": 22.730117806640624,
                    "tflops": 0.4915936146420526
                },
                {
                    "batch_size": 512,
                    "latency_ms": 11.350264785156249,
                    "tflops": 0.4934005430807189
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 5.673071689453125,
                    "tflops": 0.48834628260132235
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 2.834789705078125,
                    "tflops": 0.4936019920594597
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 1.418526666748047,
                    "tflops": 0.494593013596708
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.7145175725097657,
                    "tflops": 0.4897878795477063
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.3575027693481445,
                    "tflops": 0.4893308711454544
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 0.17815170823669435,
                    "tflops": 0.49206024883801497
                },
                {
                    "batch_size": 65536,
                    "latency_ms": 0.08905248976135255,
                    "tflops": 0.4919850849106008
                },
                {
                    "batch_size": 131072,
                    "latency_ms": 0.044440427776336666,
                    "tflops": 0.4931277944677138
                },
                {
                    "batch_size": 262144,
                    "latency_ms": 0.02215085595703125,
                    "tflops": 0.49380635870815
                },
                {
                    "batch_size": 524288,
                    "latency_ms": 0.01106384661102295,
                    "tflops": 0.49473853875281915
                },
                {
                    "batch_size": 1048576,
                    "latency_ms": 0.005518677999973297,
                    "tflops": 0.4956225617979763
                },
                {
                    "batch_size": 2097152,
                    "latency_ms": 0.0027627260355949403,
                    "tflops": 0.49593691252381683
                },
                {
                    "batch_size": 4194304,
                    "latency_ms": 0.001382732920527458,
                    "tflops": 0.49449054521469826
                },
                {
                    "batch_size": 8388608,
                    "latency_ms": 0.000693394360780716,
                    "tflops": 0.49340272093104565
                },
                {
                    "batch_size": 16777216,
                    "latency_ms": 0.00034771417516469953,
                    "tflops": 0.4905387619274846
                },
                {
                    "batch_size": 33554432,
                    "latency_ms": 0.0001725610503554344,
                    "tflops": 0.49536690202669154
                },
                {
                    "batch_size": 67108864,
                    "latency_ms": 8.607357310503721e-05,
                    "tflops": 0.49678567528585454
                },
                {
                    "batch_size": 134217728,
                    "latency_ms": 4.3041980870068073e-05,
                    "tflops": 0.497035178945151
                },
                {
                    "batch_size": 268435456,
                    "latency_ms": 2.1446032609790564e-05,
                    "tflops": 0.49743302946421764
                },
                {
                    "batch_size": 536870912,
                    "latency_ms": 1.0734852129593491e-05,
                    "tflops": 0.49652452677493114
                }
            ],
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "squeezenet1_1",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 3.786038,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 1.8983807499999998,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.007684625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.52162825,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.2602305625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.1424680625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.1220557890625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.111588109375,
                    "tflops": 5.868695412636223
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.107912634765625,
                    "tflops": 6.548630025579712
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.10468421484375,
                    "tflops": 6.85603086553467
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.1018836630859375,
                    "tflops": 7.001756402924184
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 338.00 MiB (GPU 0; 39.59 GiB total capacity; 36.93 GiB already allocated; 149.94 MiB free; 37.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 1024
        }
    },
    {
        "name": "tacotron2",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "StopIteration",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 287.55116350000003,
                    "tflops": 0.06844607433758935
                },
                {
                    "batch_size": 2,
                    "latency_ms": 443.60157649999996,
                    "tflops": 0.13959506906996055
                },
                {
                    "batch_size": 4,
                    "latency_ms": 226.317396375,
                    "tflops": 0.2718593956037263
                },
                {
                    "batch_size": 8,
                    "latency_ms": 137.217718875,
                    "tflops": 1.26744101383799
                },
                {
                    "batch_size": 16,
                    "latency_ms": 71.53687834375,
                    "tflops": 1.3461556499814933
                },
                {
                    "batch_size": 32,
                    "latency_ms": 36.182006171875,
                    "tflops": 1.4869893749182386
                },
                {
                    "batch_size": 64,
                    "latency_ms": 18.75954703125,
                    "tflops": 2.661913920767579
                },
                {
                    "batch_size": 128,
                    "latency_ms": 9.9708249921875,
                    "tflops": 5.133121777817487
                }
            ],
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "timm_efficientdet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 78.84925799999999,
                    "tflops": 0.30878823302797254
                },
                {
                    "batch_size": 2,
                    "latency_ms": 47.318130749999995,
                    "tflops": 0.47558151569495616
                },
                {
                    "batch_size": 4,
                    "latency_ms": 32.00021475,
                    "tflops": 0.7542320699971984
                },
                {
                    "batch_size": 8,
                    "latency_ms": 25.422926,
                    "tflops": 0.8936952289753469
                },
                {
                    "batch_size": 16,
                    "latency_ms": 24.290495125,
                    "tflops": 0.989075473965832
                },
                {
                    "batch_size": 32,
                    "latency_ms": 22.8286976875,
                    "tflops": 1.0675055483194764
                },
                {
                    "batch_size": 64,
                    "latency_ms": 19.7118205546875,
                    "tflops": 1.2191528685247635
                },
                {
                    "batch_size": 128,
                    "latency_ms": 11.93157778515625,
                    "tflops": 1.597939090782077
                },
                {
                    "batch_size": 256,
                    "latency_ms": 5.856252025390624,
                    "tflops": 1.6161314515183354
                },
                {
                    "batch_size": 512,
                    "latency_ms": 2.951699462890625,
                    "tflops": 1.5978087405231798
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 1.485291707519531,
                    "tflops": 1.598758832065651
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.7384513989257813,
                    "tflops": 1.5782617600758628
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.3702243302001953,
                    "tflops": 1.618597750140559
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.18351901751708985,
                    "tflops": 1.6013148588438233
                },
                {
                    "batch_size": 16384,
                    "latency_ms": 0.09087132009887695,
                    "tflops": 1.6366641279800092
                },
                {
                    "batch_size": 32768,
                    "latency_ms": 0.046809357543945315,
                    "tflops": 1.6035212166567134
                },
                {
                    "batch_size": 65536,
                    "latency_ms": 0.02350955647277832,
                    "tflops": 1.5783420337657417
                },
                {
                    "batch_size": 131072,
                    "latency_ms": 0.011586888046264648,
                    "tflops": 1.6195996561986696
                },
                {
                    "batch_size": 262144,
                    "latency_ms": 0.005826308460235596,
                    "tflops": 1.6068589748998694
                },
                {
                    "batch_size": 524288,
                    "latency_ms": 0.002901054631233215,
                    "tflops": 1.601012105090428
                },
                {
                    "batch_size": 1048576,
                    "latency_ms": 0.001440530333995819,
                    "tflops": 1.6291112986943421
                },
                {
                    "batch_size": 2097152,
                    "latency_ms": 0.0007276543526649475,
                    "tflops": 1.5895059647430476
                },
                {
                    "batch_size": 4194304,
                    "latency_ms": 0.00037389610898494723,
                    "tflops": 1.5403011463499336
                },
                {
                    "batch_size": 8388608,
                    "latency_ms": 0.00019073348385095594,
                    "tflops": 1.5309715884617427
                },
                {
                    "batch_size": 16777216,
                    "latency_ms": 0.00010026454430818557,
                    "tflops": 1.4369400222020352
                },
                {
                    "batch_size": 33554432,
                    "latency_ms": 5.6555146753788e-05,
                    "tflops": 1.2950859317519887
                },
                {
                    "batch_size": 67108864,
                    "latency_ms": 3.3719756163656715e-05,
                    "tflops": 1.078659310285337
                },
                {
                    "batch_size": 134217728,
                    "latency_ms": 2.2730711795389653e-05,
                    "tflops": 0.8021039114618702
                },
                {
                    "batch_size": 268435456,
                    "latency_ms": 1.7311461413279178e-05,
                    "tflops": 0.5218440678128071
                },
                {
                    "batch_size": 536870912,
                    "latency_ms": 1.4346970341168345e-05,
                    "tflops": 0.31647419252529635
                }
            ],
            "optimal_tflops_bs": 16384
        }
    },
    {
        "name": "timm_efficientnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 11.195264,
                    "tflops": 0.11892488167847205
                },
                {
                    "batch_size": 2,
                    "latency_ms": 5.761641,
                    "tflops": 0.2530976479037349
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.7845352500000002,
                    "tflops": 0.49727279039864375
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.438331125,
                    "tflops": 0.918175992361001
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.886412625,
                    "tflops": 1.4945171503077448
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.37313515625,
                    "tflops": 3.497221934997921
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.317694359375,
                    "tflops": 4.500715675325339
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.2878242890625,
                    "tflops": 5.100334749279101
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.268805078125,
                    "tflops": 5.405539536248824
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.25937370507812496,
                    "tflops": 5.6496133482430215
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.2540152763671875,
                    "tflops": 5.8406287571144375
                }
            ],
            "error_message": "cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.",
            "optimal_tflops_bs": 1024
        }
    },
    {
        "name": "timm_nfnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 20.435257999999997,
                    "tflops": 2.60343840483197
                },
                {
                    "batch_size": 2,
                    "latency_ms": 10.31688475,
                    "tflops": 3.204897645235483
                },
                {
                    "batch_size": 4,
                    "latency_ms": 5.2193515,
                    "tflops": 4.260016823739674
                },
                {
                    "batch_size": 8,
                    "latency_ms": 3.2782248125,
                    "tflops": 5.3250771108103825
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.796151625,
                    "tflops": 7.8704789358939635
                },
                {
                    "batch_size": 32,
                    "latency_ms": 1.4029939062499999,
                    "tflops": 9.642140110255847
                },
                {
                    "batch_size": 64,
                    "latency_ms": 1.2217034140625,
                    "tflops": 10.81119063960716
                },
                {
                    "batch_size": 128,
                    "latency_ms": 1.12275669140625,
                    "tflops": 11.709363175853879
                },
                {
                    "batch_size": 256,
                    "latency_ms": 1.071379056640625,
                    "tflops": 12.321638908928788
                },
                {
                    "batch_size": 512,
                    "latency_ms": 1.050383853515625,
                    "tflops": 12.53873430955084
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 1.0424402167968752,
                    "tflops": 12.646710378958659
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 1.0378369050292968,
                    "tflops": 12.706470442127033
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 9.00 GiB (GPU 0; 39.59 GiB total capacity; 24.46 GiB already allocated; 7.05 GiB free; 30.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 2048
        }
    },
    {
        "name": "timm_regnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 17.8620995,
                    "tflops": 1.6631587833423371
                },
                {
                    "batch_size": 2,
                    "latency_ms": 9.760102,
                    "tflops": 2.957817619019055
                },
                {
                    "batch_size": 4,
                    "latency_ms": 5.74170775,
                    "tflops": 4.94664655302915
                },
                {
                    "batch_size": 8,
                    "latency_ms": 4.5898054375,
                    "tflops": 6.461162365409526
                },
                {
                    "batch_size": 16,
                    "latency_ms": 3.2269268125,
                    "tflops": 8.688760486086593
                },
                {
                    "batch_size": 32,
                    "latency_ms": 2.4752896250000003,
                    "tflops": 11.199050152927555
                },
                {
                    "batch_size": 64,
                    "latency_ms": 3.49936884375,
                    "tflops": 7.989153622534193
                },
                {
                    "batch_size": 128,
                    "latency_ms": 3.82601181640625,
                    "tflops": 6.97505234845339
                },
                {
                    "batch_size": 256,
                    "latency_ms": 3.2335213222656254,
                    "tflops": 8.051081620418387
                },
                {
                    "batch_size": 512,
                    "latency_ms": 2.4779215732421873,
                    "tflops": 10.49673711696824
                }
            ],
            "error_message": "cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.",
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "timm_resnest",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 5.214389000000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.7896235,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.602039375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.0844584375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.7879742812499999,
                    "tflops": 5.208685753301572
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.470262921875,
                    "tflops": 8.124521968321629
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.43195000781250004,
                    "tflops": 9.289302377417354
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.41483984375000005,
                    "tflops": 9.814764444399323
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.40405279296875,
                    "tflops": 10.186695273395909
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.398072474609375,
                    "tflops": 10.383065176584434
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.39476463623046876,
                    "tflops": 10.572757358968216
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.42653172802734374,
                    "tflops": 11.785648765283248
                }
            ],
            "error_message": "cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.",
            "optimal_tflops_bs": 2048
        }
    },
    {
        "name": "timm_vision_transformer",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 9.149358,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 4.7134345,
                    "tflops": 2.533771592463321
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.760589125,
                    "tflops": 4.374028659924673
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.31368,
                    "tflops": 9.034370692810995
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.0437202187499999,
                    "tflops": 11.558864423310292
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.99769628125,
                    "tflops": 13.088255421112814
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.9213050859375,
                    "tflops": 14.329113159940515
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.8812565625,
                    "tflops": 15.108790592934053
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.85629894921875,
                    "tflops": 15.578231510935082
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.8470299462890625,
                    "tflops": 15.817703235786963
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.8422308837890625,
                    "tflops": 15.934744166459591
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.840359185546875,
                    "tflops": 15.98079505537795
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.8400323143310546,
                    "tflops": 15.993771359707097
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 9.23 GiB (GPU 0; 39.59 GiB total capacity; 20.84 GiB already allocated; 9.11 GiB free; 28.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 4096
        }
    },
    {
        "name": "timm_vovnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 7.4571445,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 3.8369074999999997,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.117337875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.233197,
                    "tflops": 9.474274014259922
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.9606997500000001,
                    "tflops": 9.78026972374692
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.851900125,
                    "tflops": 11.603658282748095
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.8217771015625,
                    "tflops": 12.663804617534339
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.85648766796875,
                    "tflops": 13.623956967953564
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.79883475390625,
                    "tflops": 14.014388807023911
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.7963121435546875,
                    "tflops": 14.27014150822401
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.8090550268554688,
                    "tflops": 14.499460844454092
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 18.38 GiB (GPU 0; 39.59 GiB total capacity; 19.61 GiB already allocated; 1.77 GiB free; 35.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 1024
        }
    },
    {
        "name": "tts_angular",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 4.50803,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.3315520000000003,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.1712562499999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.781214875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.40743687500000003,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.223894046875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.1546983359375,
                    "tflops": 7.107817788842051
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.11432269140625,
                    "tflops": 9.285290196732785
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.098875779296875,
                    "tflops": 11.723284101884841
                },
                {
                    "batch_size": 512,
                    "latency_ms": 0.08420588378906249,
                    "tflops": 14.023581948260944
                },
                {
                    "batch_size": 1024,
                    "latency_ms": 0.08052595654296875,
                    "tflops": 14.917817893345866
                },
                {
                    "batch_size": 2048,
                    "latency_ms": 0.07698791967773438,
                    "tflops": 15.804059243822437
                },
                {
                    "batch_size": 4096,
                    "latency_ms": 0.07354796813964844,
                    "tflops": 16.556116045232347
                },
                {
                    "batch_size": 8192,
                    "latency_ms": 0.07120384741210938,
                    "tflops": 17.133970429112207
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 11.72 GiB (GPU 0; 39.59 GiB total capacity; 30.43 GiB already allocated; 2.69 GiB free; 34.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 8192
        }
    },
    {
        "name": "vgg16",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 3.573994,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.5131585000000003,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.97584775,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.70227175,
                    "tflops": 11.564367695957271
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.2039846875,
                    "tflops": 11.078175673306818
                },
                {
                    "batch_size": 32,
                    "latency_ms": 1.073507078125,
                    "tflops": 11.173044966267716
                },
                {
                    "batch_size": 64,
                    "latency_ms": 1.0234669609375,
                    "tflops": 11.901611074875401
                },
                {
                    "batch_size": 128,
                    "latency_ms": 1.0007848828124999,
                    "tflops": 12.214133447192669
                },
                {
                    "batch_size": 256,
                    "latency_ms": 0.984027009765625,
                    "tflops": 12.240506711626916
                },
                {
                    "batch_size": 512,
                    "latency_ms": 1.3042630654296874,
                    "tflops": 14.221986243876279
                }
            ],
            "error_message": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
            "optimal_tflops_bs": 512
        }
    },
    {
        "name": "vision_maskrcnn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 64.020323,
                    "tflops": 5.835317921948088
                },
                {
                    "batch_size": 2,
                    "latency_ms": 60.962219,
                    "tflops": 6.602749730754922
                },
                {
                    "batch_size": 4,
                    "latency_ms": 81.29807087500001,
                    "tflops": 8.766240328425482
                },
                {
                    "batch_size": 8,
                    "latency_ms": 66.6745873125,
                    "tflops": 8.207298225967396
                },
                {
                    "batch_size": 16,
                    "latency_ms": 58.704854874999995,
                    "tflops": 8.438202407541885
                },
                {
                    "batch_size": 32,
                    "latency_ms": 63.923564890625,
                    "tflops": 8.449167241563293
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 27.51 GiB already allocated; 335.94 MiB free; 36.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "yolov3",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 14.918198499999999,
                    "tflops": 3.5273503605915635
                },
                {
                    "batch_size": 2,
                    "latency_ms": 8.6161355,
                    "tflops": 6.03348513387168
                },
                {
                    "batch_size": 4,
                    "latency_ms": 6.165743375,
                    "tflops": 9.585793723231468
                },
                {
                    "batch_size": 8,
                    "latency_ms": 5.302622187500001,
                    "tflops": 11.565965583047351
                },
                {
                    "batch_size": 16,
                    "latency_ms": 4.01424659375,
                    "tflops": 10.853075484354557
                },
                {
                    "batch_size": 32,
                    "latency_ms": 3.72250459375,
                    "tflops": 11.507353895019996
                },
                {
                    "batch_size": 64,
                    "latency_ms": 3.6135609296875,
                    "tflops": 11.968966197873199
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 39.59 GiB total capacity; 35.95 GiB already allocated; 153.94 MiB free; 37.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_tflops_bs": 64
        }
    }
]