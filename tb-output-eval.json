[
    {
        "name": "BERT_pytorch",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 19.942054499999998,
                    "tflops": 1.1404928678636244
                },
                {
                    "batch_size": 2,
                    "latency_ms": 10.363331500000001,
                    "tflops": 3.2783887325997747
                },
                {
                    "batch_size": 4,
                    "latency_ms": 5.398301,
                    "tflops": 6.264861268621552
                },
                {
                    "batch_size": 8,
                    "latency_ms": 3.0119165,
                    "tflops": 11.288128328845685
                },
                {
                    "batch_size": 16,
                    "latency_ms": 2.63411740625,
                    "tflops": 13.16586374330967
                },
                {
                    "batch_size": 32,
                    "latency_ms": 2.49390140625,
                    "tflops": 14.132452409116928
                },
                {
                    "batch_size": 64,
                    "latency_ms": 2.42458446875,
                    "tflops": 14.378771370032553
                },
                {
                    "batch_size": 128,
                    "latency_ms": 2.34514152734375,
                    "tflops": 14.85528453494099
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "Background_Matting",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "LearningToPaint",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 1406.3774955,
                    "tflops": 0.41235853890990226
                },
                {
                    "batch_size": 2,
                    "latency_ms": 716.1359372500001,
                    "tflops": 1.0714891929422419
                },
                {
                    "batch_size": 4,
                    "latency_ms": 376.836218875,
                    "tflops": 0.8849345543835146
                },
                {
                    "batch_size": 8,
                    "latency_ms": 187.4740889375,
                    "tflops": 1.6279845880486368
                },
                {
                    "batch_size": 16,
                    "latency_ms": 104.3679234375,
                    "tflops": 3.2386800632316652
                },
                {
                    "batch_size": 32,
                    "latency_ms": 55.866405953124996,
                    "tflops": 4.401010903077569
                },
                {
                    "batch_size": 64,
                    "latency_ms": 35.83206475,
                    "tflops": 6.257247089123736
                },
                {
                    "batch_size": 128,
                    "latency_ms": 28.73784662109375,
                    "tflops": 7.246068439184716
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "Super_SloMo",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 43.1989715,
                    "tflops": 9.526472435769263
                },
                {
                    "batch_size": 2,
                    "latency_ms": 35.02871275,
                    "tflops": 11.220576001202064
                },
                {
                    "batch_size": 4,
                    "latency_ms": 29.383122,
                    "tflops": 13.522288712609466
                },
                {
                    "batch_size": 8,
                    "latency_ms": 27.1573675625,
                    "tflops": 14.593838183832665
                },
                {
                    "batch_size": 16,
                    "latency_ms": 16.823498625,
                    "tflops": 14.697088942689897
                },
                {
                    "batch_size": 32,
                    "latency_ms": 8.407216359375,
                    "tflops": 14.70865302211985
                },
                {
                    "batch_size": 64,
                    "latency_ms": 4.2064367421875,
                    "tflops": 14.701489080739611
                },
                {
                    "batch_size": 128,
                    "latency_ms": 2.1022580625,
                    "tflops": 14.699247346065725
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "alexnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 1.646734,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 0.8182897499999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.493954875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.24780125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.13957740624999998,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.093695328125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.07567650781249999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.06255775390625,
                    "tflops": 0.0
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "attention_is_all_you_need_pytorch",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 18.355701,
                    "tflops": 0.26482268845358764
                },
                {
                    "batch_size": 2,
                    "latency_ms": 9.62868475,
                    "tflops": 0.222993883202299
                },
                {
                    "batch_size": 4,
                    "latency_ms": 4.9425842499999995,
                    "tflops": 0.731223762894321
                },
                {
                    "batch_size": 8,
                    "latency_ms": 2.836573875,
                    "tflops": 1.0779364903379784
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.234826,
                    "tflops": 2.750868485099597
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.5764170468750001,
                    "tflops": 5.36897688054954
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.30618851562500005,
                    "tflops": 11.009354008547863
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.3332036328125,
                    "tflops": 13.842541134047634
                }
            ],
            "optimal_latency_bs": 64,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "dcgan",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 1.5030895000000002,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 0.70255825,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.351735,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.19135875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.09750215625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.067354890625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.04638676562500001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.0394037265625,
                    "tflops": 0.0
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "demucs",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 30.7452585,
                    "tflops": 4.401300404666741
                },
                {
                    "batch_size": 2,
                    "latency_ms": 19.15264825,
                    "tflops": 7.146581352176958
                },
                {
                    "batch_size": 4,
                    "latency_ms": 14.677054125000002,
                    "tflops": 9.419538166476437
                },
                {
                    "batch_size": 8,
                    "latency_ms": 11.8246958125,
                    "tflops": 11.563327363126076
                },
                {
                    "batch_size": 16,
                    "latency_ms": 11.661739687499999,
                    "tflops": 12.864189423355688
                },
                {
                    "batch_size": 32,
                    "latency_ms": 9.648495593749999,
                    "tflops": 13.928816109556838
                },
                {
                    "batch_size": 64,
                    "latency_ms": 8.151420546875,
                    "tflops": 14.914896565905376
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 2.83 GiB (GPU 0; 39.59 GiB total capacity; 35.95 GiB already allocated; 529.94 MiB free; 36.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 64,
            "optimal_tflops_bs": 64
        }
    },
    {
        "name": "densenet121",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 24.4970715,
                    "tflops": 0.2591120784921976
                },
                {
                    "batch_size": 2,
                    "latency_ms": 13.41027,
                    "tflops": 0.4873350575601143
                },
                {
                    "batch_size": 4,
                    "latency_ms": 6.495881875,
                    "tflops": 0.9345335370747335
                },
                {
                    "batch_size": 8,
                    "latency_ms": 3.2874646875,
                    "tflops": 1.879812621765671
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.5606259062499999,
                    "tflops": 3.881752798787379
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.9001107187499999,
                    "tflops": 6.676445677133522
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.7525592421875,
                    "tflops": 8.665503849574428
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.709635953125,
                    "tflops": 9.226686490713396
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_101_c4",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 134.8016215,
                    "tflops": 12.414473415106531
                },
                {
                    "batch_size": 2,
                    "latency_ms": 131.01085875,
                    "tflops": 12.94494763832239
                },
                {
                    "batch_size": 4,
                    "latency_ms": 118.542852375,
                    "tflops": 13.31043046677754
                },
                {
                    "batch_size": 8,
                    "latency_ms": 118.9641598125,
                    "tflops": 13.357095201145773
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 5.00 GiB (GPU 0; 39.59 GiB total capacity; 23.11 GiB already allocated; 3.21 GiB free; 33.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 4,
            "optimal_tflops_bs": 8
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_101_dc5",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 78.5206,
                    "tflops": 11.797337255688621
                },
                {
                    "batch_size": 2,
                    "latency_ms": 77.144316,
                    "tflops": 12.011300538487736
                },
                {
                    "batch_size": 4,
                    "latency_ms": 83.80636725,
                    "tflops": 13.630684967873169
                },
                {
                    "batch_size": 8,
                    "latency_ms": 83.07447487499999,
                    "tflops": 13.921322108899323
                },
                {
                    "batch_size": 16,
                    "latency_ms": 89.84658584375,
                    "tflops": 14.185468384415556
                },
                {
                    "batch_size": 32,
                    "latency_ms": 71.60926120312499,
                    "tflops": 14.286545062639766
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.81 GiB (GPU 0; 39.59 GiB total capacity; 19.97 GiB already allocated; 4.36 GiB free; 32.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 32,
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_101_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 51.175786,
                    "tflops": 9.701148100131375
                },
                {
                    "batch_size": 2,
                    "latency_ms": 45.90344625,
                    "tflops": 10.857421966956762
                },
                {
                    "batch_size": 4,
                    "latency_ms": 67.08994849999999,
                    "tflops": 12.594017656070294
                },
                {
                    "batch_size": 8,
                    "latency_ms": 56.403741625,
                    "tflops": 12.562294425346215
                },
                {
                    "batch_size": 16,
                    "latency_ms": 55.2704871875,
                    "tflops": 12.563685137078432
                },
                {
                    "batch_size": 32,
                    "latency_ms": 53.771087593749996,
                    "tflops": 12.593071745490132
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.86 GiB already allocated; 6.09 GiB free; 31.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 2,
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_50_c4",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 122.00035449999999,
                    "tflops": 12.538769536790758
                },
                {
                    "batch_size": 2,
                    "latency_ms": 119.369685,
                    "tflops": 13.040747807922035
                },
                {
                    "batch_size": 4,
                    "latency_ms": 103.45254325,
                    "tflops": 13.221228385511218
                },
                {
                    "batch_size": 8,
                    "latency_ms": 95.60995199999999,
                    "tflops": 13.20919086939949
                },
                {
                    "batch_size": 16,
                    "latency_ms": 87.2496280625,
                    "tflops": 13.15441986411144
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 8.70 GiB (GPU 0; 39.59 GiB total capacity; 30.74 GiB already allocated; 3.39 GiB free; 33.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 16,
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_50_dc5",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 65.67854750000001,
                    "tflops": 11.97226589640324
                },
                {
                    "batch_size": 2,
                    "latency_ms": 65.36254875,
                    "tflops": 12.14014998548326
                },
                {
                    "batch_size": 4,
                    "latency_ms": 69.810530875,
                    "tflops": 13.5625418610207
                },
                {
                    "batch_size": 8,
                    "latency_ms": 65.99099287499999,
                    "tflops": 14.021834569150844
                },
                {
                    "batch_size": 16,
                    "latency_ms": 71.50336959375,
                    "tflops": 13.948230246752654
                },
                {
                    "batch_size": 32,
                    "latency_ms": 56.0354356875,
                    "tflops": 14.268002214914887
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.81 GiB (GPU 0; 39.59 GiB total capacity; 19.91 GiB already allocated; 6.12 GiB free; 31.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 32,
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "detectron2_fasterrcnn_r_50_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 37.587809,
                    "tflops": 9.096978723312477
                },
                {
                    "batch_size": 2,
                    "latency_ms": 33.800323,
                    "tflops": 10.2254444725944
                },
                {
                    "batch_size": 4,
                    "latency_ms": 51.67394875,
                    "tflops": 12.367242574359564
                },
                {
                    "batch_size": 8,
                    "latency_ms": 41.833656875,
                    "tflops": 11.881809145078865
                },
                {
                    "batch_size": 16,
                    "latency_ms": 38.939593593750004,
                    "tflops": 11.58034512373403
                },
                {
                    "batch_size": 32,
                    "latency_ms": 40.079189734375,
                    "tflops": 12.04435838112757
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.79 GiB already allocated; 6.34 GiB free; 30.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 2,
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "detectron2_fcos_r_50_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 40.401933,
                    "tflops": 8.153426597051135
                },
                {
                    "batch_size": 2,
                    "latency_ms": 37.655975749999996,
                    "tflops": 9.307855563778489
                },
                {
                    "batch_size": 4,
                    "latency_ms": 43.17390225,
                    "tflops": 11.137022267132176
                },
                {
                    "batch_size": 8,
                    "latency_ms": 42.045187,
                    "tflops": 11.36580013578196
                },
                {
                    "batch_size": 16,
                    "latency_ms": 40.844771,
                    "tflops": 10.697114717312616
                },
                {
                    "batch_size": 32,
                    "latency_ms": 37.5718385,
                    "tflops": 9.979406881423515
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 19.64 GiB already allocated; 6.76 GiB free; 30.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 32,
            "optimal_tflops_bs": 8
        }
    },
    {
        "name": "detectron2_maskrcnn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 4020.2602895,
                    "tflops": 8.902988505469288
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2043.98982575,
                    "tflops": 10.364269952120512
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1163.22848625,
                    "tflops": 11.692983847594917
                },
                {
                    "batch_size": 8,
                    "latency_ms": 577.7997990625,
                    "tflops": 11.70268977511144
                },
                {
                    "batch_size": 16,
                    "latency_ms": 263.442526625,
                    "tflops": 11.318939874339916
                },
                {
                    "batch_size": 32,
                    "latency_ms": 139.66817040625,
                    "tflops": 11.80281187247957
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.80 GiB already allocated; 6.22 GiB free; 30.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 32,
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "detectron2_maskrcnn_r_101_c4",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 145.048811,
                    "tflops": 12.356560946712928
                },
                {
                    "batch_size": 2,
                    "latency_ms": 140.42465075,
                    "tflops": 12.752793550228166
                },
                {
                    "batch_size": 4,
                    "latency_ms": 126.12264487499999,
                    "tflops": 13.215365034351935
                },
                {
                    "batch_size": 8,
                    "latency_ms": 125.981439,
                    "tflops": 13.327201941920608
                },
                {
                    "batch_size": 16,
                    "latency_ms": 113.53660009375,
                    "tflops": 14.159572538977933
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 10.32 GiB (GPU 0; 39.59 GiB total capacity; 25.76 GiB already allocated; 7.55 GiB free; 29.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 16,
            "optimal_tflops_bs": 16
        }
    },
    {
        "name": "detectron2_maskrcnn_r_101_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 56.351429499999995,
                    "tflops": 9.297153135655355
                },
                {
                    "batch_size": 2,
                    "latency_ms": 49.6675245,
                    "tflops": 10.410353361959064
                },
                {
                    "batch_size": 4,
                    "latency_ms": 69.61581025000001,
                    "tflops": 12.535458244429602
                },
                {
                    "batch_size": 8,
                    "latency_ms": 59.0025914375,
                    "tflops": 12.283690201896903
                },
                {
                    "batch_size": 16,
                    "latency_ms": 57.327249531250004,
                    "tflops": 12.430325952772506
                },
                {
                    "batch_size": 32,
                    "latency_ms": 56.3441073125,
                    "tflops": 12.334488361995678
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.87 GiB already allocated; 6.00 GiB free; 31.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 2,
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "detectron2_maskrcnn_r_50_c4",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 131.6627905,
                    "tflops": 12.628249728311713
                },
                {
                    "batch_size": 2,
                    "latency_ms": 128.77846775,
                    "tflops": 12.765670575565286
                },
                {
                    "batch_size": 4,
                    "latency_ms": 111.883674,
                    "tflops": 13.000602244393784
                },
                {
                    "batch_size": 8,
                    "latency_ms": 102.489488625,
                    "tflops": 13.115616250973472
                },
                {
                    "batch_size": 16,
                    "latency_ms": 92.39210075,
                    "tflops": 13.01546490323298
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 8.63 GiB (GPU 0; 39.59 GiB total capacity; 30.51 GiB already allocated; 3.60 GiB free; 33.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 16,
            "optimal_tflops_bs": 8
        }
    },
    {
        "name": "detectron2_maskrcnn_r_50_fpn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 42.8446775,
                    "tflops": 8.851176134464684
                },
                {
                    "batch_size": 2,
                    "latency_ms": 37.97127175,
                    "tflops": 9.798920149797459
                },
                {
                    "batch_size": 4,
                    "latency_ms": 55.093277125,
                    "tflops": 12.030234308979713
                },
                {
                    "batch_size": 8,
                    "latency_ms": 44.981372375,
                    "tflops": 11.590673024017981
                },
                {
                    "batch_size": 16,
                    "latency_ms": 41.3578170625,
                    "tflops": 11.30020368855442
                },
                {
                    "batch_size": 32,
                    "latency_ms": 42.83133478125,
                    "tflops": 11.736172814980451
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 22.80 GiB already allocated; 6.18 GiB free; 30.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 2,
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "dlrm",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 2.1136105,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 1.04018225,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.52842375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.2749315,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.13560543749999998,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.07445328125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.0355756015625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.01762054296875,
                    "tflops": 0.0
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "drq",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "fambench_dlrm",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "TypeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "\n'numpy.int64' object in attribute 'EmbeddingBag.num_embeddings' is not a valid constant.\nValid constants are:\n1. a nn.ModuleList\n2. a value of type {bool, float, int, str, NoneType, torch.device, torch.layout, torch.dtype}\n3. a list or tuple of (2)\n"
        }
    },
    {
        "name": "fambench_xlmr",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 20.660407499999998,
                    "tflops": 2.527563760263431
                },
                {
                    "batch_size": 2,
                    "latency_ms": 15.3966535,
                    "tflops": 3.409395758844098
                },
                {
                    "batch_size": 4,
                    "latency_ms": 5.497052500000001,
                    "tflops": 9.330871938245476
                },
                {
                    "batch_size": 8,
                    "latency_ms": 4.03515475,
                    "tflops": 12.934929825316244
                },
                {
                    "batch_size": 16,
                    "latency_ms": 3.41798290625,
                    "tflops": 15.364993279542652
                },
                {
                    "batch_size": 32,
                    "latency_ms": 3.2627145,
                    "tflops": 16.194777956295496
                },
                {
                    "batch_size": 64,
                    "latency_ms": 3.14615778125,
                    "tflops": 16.686866052332963
                },
                {
                    "batch_size": 128,
                    "latency_ms": 3.097501765625,
                    "tflops": 16.950800977693127
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "fastNLP_Bert",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 60.699638500000006,
                    "tflops": 8.571727025917902
                },
                {
                    "batch_size": 2,
                    "latency_ms": 46.0747795,
                    "tflops": 10.940152220072575
                },
                {
                    "batch_size": 4,
                    "latency_ms": 43.058834125000004,
                    "tflops": 11.719387444720041
                },
                {
                    "batch_size": 8,
                    "latency_ms": 39.447811375,
                    "tflops": 12.756377109759315
                },
                {
                    "batch_size": 16,
                    "latency_ms": 37.7049525,
                    "tflops": 13.282978202656908
                },
                {
                    "batch_size": 32,
                    "latency_ms": 36.635467671875006,
                    "tflops": 13.68169997214604
                },
                {
                    "batch_size": 64,
                    "latency_ms": 35.581434203125,
                    "tflops": 14.05676363553548
                },
                {
                    "batch_size": 128,
                    "latency_ms": 34.8307572421875,
                    "tflops": 14.350663227940336
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "hf_Albert",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 14.3228845,
                    "tflops": 9.167660549496953
                },
                {
                    "batch_size": 2,
                    "latency_ms": 11.749732250000001,
                    "tflops": 11.575816464601473
                },
                {
                    "batch_size": 4,
                    "latency_ms": 11.100821374999999,
                    "tflops": 12.565185769276827
                },
                {
                    "batch_size": 8,
                    "latency_ms": 10.5413203125,
                    "tflops": 13.362943861239051
                },
                {
                    "batch_size": 16,
                    "latency_ms": 10.20611496875,
                    "tflops": 13.665473809629688
                },
                {
                    "batch_size": 32,
                    "latency_ms": 9.850202265625,
                    "tflops": 14.178956271272181
                },
                {
                    "batch_size": 64,
                    "latency_ms": 9.5636915625,
                    "tflops": 14.610960409017604
                },
                {
                    "batch_size": 128,
                    "latency_ms": 9.425963890624999,
                    "tflops": 14.827708406709231
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "hf_Bart",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 20.1366365,
                    "tflops": 10.193410399426492
                },
                {
                    "batch_size": 2,
                    "latency_ms": 16.00853725,
                    "tflops": 13.245847302616125
                },
                {
                    "batch_size": 4,
                    "latency_ms": 14.812066125000001,
                    "tflops": 14.343604142522555
                },
                {
                    "batch_size": 8,
                    "latency_ms": 14.035424625000001,
                    "tflops": 15.341820518181331
                },
                {
                    "batch_size": 16,
                    "latency_ms": 13.6803355,
                    "tflops": 15.625230923337048
                },
                {
                    "batch_size": 32,
                    "latency_ms": 13.241880515624999,
                    "tflops": 16.17149281090452
                },
                {
                    "batch_size": 64,
                    "latency_ms": 12.9691549375,
                    "tflops": 16.494962561166663
                },
                {
                    "batch_size": 128,
                    "latency_ms": 12.81011944140625,
                    "tflops": 16.723190101298336
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "hf_Bert",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 15.284447499999999,
                    "tflops": 10.209866326502423
                },
                {
                    "batch_size": 2,
                    "latency_ms": 12.71618825,
                    "tflops": 12.755440144737886
                },
                {
                    "batch_size": 4,
                    "latency_ms": 11.593426000000001,
                    "tflops": 14.162166464292369
                },
                {
                    "batch_size": 8,
                    "latency_ms": 11.0684854375,
                    "tflops": 15.063242725382482
                },
                {
                    "batch_size": 16,
                    "latency_ms": 10.70414115625,
                    "tflops": 15.485694973412322
                },
                {
                    "batch_size": 32,
                    "latency_ms": 10.364657109374999,
                    "tflops": 16.01335559977268
                },
                {
                    "batch_size": 64,
                    "latency_ms": 10.08454446875,
                    "tflops": 16.453017312539952
                },
                {
                    "batch_size": 128,
                    "latency_ms": 9.953808972656251,
                    "tflops": 16.68224255201443
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "hf_BigBird",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 164.5090195,
                    "tflops": 10.379112702128996
                },
                {
                    "batch_size": 2,
                    "latency_ms": 144.67052775,
                    "tflops": 11.728378560587394
                },
                {
                    "batch_size": 4,
                    "latency_ms": 131.994881625,
                    "tflops": 12.897162669321206
                },
                {
                    "batch_size": 8,
                    "latency_ms": 125.31043787499999,
                    "tflops": 13.645887059316346
                },
                {
                    "batch_size": 16,
                    "latency_ms": 123.5169495625,
                    "tflops": 13.883464412986111
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 24.59 GiB (GPU 0; 39.59 GiB total capacity; 1.23 GiB already allocated; 12.73 GiB free; 25.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 16,
            "optimal_tflops_bs": 16
        }
    },
    {
        "name": "hf_DistilBert",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 8.937453,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 7.3615775,
                    "tflops": 12.697106956549334
                },
                {
                    "batch_size": 4,
                    "latency_ms": 6.834600875,
                    "tflops": 14.156361918071887
                },
                {
                    "batch_size": 8,
                    "latency_ms": 6.48818225,
                    "tflops": 15.205814237052504
                },
                {
                    "batch_size": 16,
                    "latency_ms": 6.2764604375,
                    "tflops": 15.709332303245516
                },
                {
                    "batch_size": 32,
                    "latency_ms": 6.087462125,
                    "tflops": 16.184170382665062
                },
                {
                    "batch_size": 64,
                    "latency_ms": 5.93971275,
                    "tflops": 16.57133738726424
                },
                {
                    "batch_size": 128,
                    "latency_ms": 5.86724826171875,
                    "tflops": 16.84236477148478
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "hf_GPT2",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 31.288304500000002,
                    "tflops": 12.958141868697915
                },
                {
                    "batch_size": 2,
                    "latency_ms": 29.738508500000002,
                    "tflops": 13.674199316983124
                },
                {
                    "batch_size": 4,
                    "latency_ms": 29.074686125,
                    "tflops": 13.99773546645197
                },
                {
                    "batch_size": 8,
                    "latency_ms": 28.205926875000003,
                    "tflops": 14.457697956106045
                },
                {
                    "batch_size": 16,
                    "latency_ms": 27.4184893125,
                    "tflops": 14.873536379810599
                },
                {
                    "batch_size": 32,
                    "latency_ms": 27.096356203124998,
                    "tflops": 15.104196780037656
                },
                {
                    "batch_size": 64,
                    "latency_ms": 26.886084734375,
                    "tflops": 15.243775503289548
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 24.54 GiB (GPU 0; 39.59 GiB total capacity; 14.35 GiB already allocated; 17.84 GiB free; 20.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 64,
            "optimal_tflops_bs": 64
        }
    },
    {
        "name": "hf_Longformer",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 121.709986,
                    "tflops": 13.064951337224596
                },
                {
                    "batch_size": 2,
                    "latency_ms": 121.32074449999999,
                    "tflops": 13.032012050662786
                },
                {
                    "batch_size": 4,
                    "latency_ms": 117.7916125,
                    "tflops": 13.466691355438845
                },
                {
                    "batch_size": 8,
                    "latency_ms": 115.1810585625,
                    "tflops": 13.732109285209425
                },
                {
                    "batch_size": 16,
                    "latency_ms": 113.997134375,
                    "tflops": 13.939731568281605
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 24.54 GiB (GPU 0; 39.59 GiB total capacity; 1.31 GiB already allocated; 12.23 GiB free; 25.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 16,
            "optimal_tflops_bs": 16
        }
    },
    {
        "name": "hf_Reformer",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 14.9307005,
                    "tflops": 6.904545449245057
                },
                {
                    "batch_size": 2,
                    "latency_ms": 12.13058425,
                    "tflops": 8.689932512976556
                },
                {
                    "batch_size": 4,
                    "latency_ms": 10.88033925,
                    "tflops": 9.64371596207777
                },
                {
                    "batch_size": 8,
                    "latency_ms": 10.125132375,
                    "tflops": 10.513648464039338
                },
                {
                    "batch_size": 16,
                    "latency_ms": 9.751489,
                    "tflops": 10.913502856028853
                },
                {
                    "batch_size": 32,
                    "latency_ms": 9.641665750000001,
                    "tflops": 10.998270577939802
                },
                {
                    "batch_size": 64,
                    "latency_ms": 9.5685677890625,
                    "tflops": 11.146256477294102
                },
                {
                    "batch_size": 128,
                    "latency_ms": 9.580545152343749,
                    "tflops": 11.086993453250834
                }
            ],
            "optimal_latency_bs": 64,
            "optimal_tflops_bs": 64
        }
    },
    {
        "name": "hf_T5",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 70.84384800000001,
                    "tflops": 9.10770790565337
                },
                {
                    "batch_size": 2,
                    "latency_ms": 61.1428425,
                    "tflops": 10.695213336853778
                },
                {
                    "batch_size": 4,
                    "latency_ms": 56.15040875,
                    "tflops": 11.59162550420979
                },
                {
                    "batch_size": 8,
                    "latency_ms": 55.484071562500006,
                    "tflops": 11.749861500658707
                },
                {
                    "batch_size": 16,
                    "latency_ms": 53.74951734375,
                    "tflops": 12.140055000763212
                },
                {
                    "batch_size": 32,
                    "latency_ms": 53.478667640625005,
                    "tflops": 12.230773474469856
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 39.59 GiB total capacity; 27.48 GiB already allocated; 3.67 GiB free; 34.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 32,
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "maml",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "maml_omniglot",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "mnasnet1_0",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 6.944852,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 3.458248,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.1031678749999996,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.91177525,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.47007290625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.26102821875000004,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.227763265625,
                    "tflops": 4.270849988488331
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.21170457421875,
                    "tflops": 4.81548203523098
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "mobilenet_v2",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 8.038630000000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 4.105839250000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.43182625,
                    "tflops": 0.3677356921541438
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.1302309375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.52845928125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.28463475000000005,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.2600595390625,
                    "tflops": 3.5773436621040835
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.24311037890624998,
                    "tflops": 3.9185795803769095
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "mobilenet_v2_quantized_qat",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "NotImplemented",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "The eval test only supports CPU."
        }
    },
    {
        "name": "mobilenet_v3_large",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 9.373693,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 4.86255875,
                    "tflops": 0.1608064043882158
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.82092125,
                    "tflops": 0.28465964382688874
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.2665185,
                    "tflops": 0.5711561772327612
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.62253096875,
                    "tflops": 1.1304105167635496
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.312946375,
                    "tflops": 2.1847634014193256
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.19806881250000002,
                    "tflops": 3.4409406506447135
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.1839896640625,
                    "tflops": 4.0729289910625015
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "moco",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 64.71955,
                    "tflops": 0.6877181903900845
                },
                {
                    "batch_size": 2,
                    "latency_ms": 34.126146500000004,
                    "tflops": 1.2650165076768398
                },
                {
                    "batch_size": 4,
                    "latency_ms": 17.158995750000003,
                    "tflops": 2.212839822268687
                },
                {
                    "batch_size": 8,
                    "latency_ms": 8.7752904375,
                    "tflops": 4.248844491494265
                },
                {
                    "batch_size": 16,
                    "latency_ms": 5.01143803125,
                    "tflops": 7.007174824406619
                },
                {
                    "batch_size": 32,
                    "latency_ms": 3.813082828125,
                    "tflops": 8.76855348701223
                },
                {
                    "batch_size": 64,
                    "latency_ms": 3.3162104921875004,
                    "tflops": 10.037000238807485
                },
                {
                    "batch_size": 128,
                    "latency_ms": 3.0480918828125,
                    "tflops": 10.81263069413276
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "nvidia_deeprecommender",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 1.277784,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 0.6607032500000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.374440375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.19870512499999998,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.10003,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.052798765625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.0422855625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.0379708984375,
                    "tflops": 0.0
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "opacus_cifar10",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 4.8071415,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.6127095000000002,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.22369825,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.6698659375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.34991443749999995,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.223046953125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.13391359375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.099106453125,
                    "tflops": 11.493023482979474
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "pyhpc_equation_of_state",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 4.036020499999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.0138332500000002,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.997186625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.4999709375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.273320375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.12380884375000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.0617680078125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.0461493984375,
                    "tflops": 0.0
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "pyhpc_isoneutral_mixing",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 5.0691775,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.56279875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.25611675,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.6541043125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.38206865624999997,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.19773915625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.09514208593750001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.06835884765625,
                    "tflops": 0.0
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "pyhpc_turbulent_kinetic_energy",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "IndexError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "index -2 is out of bounds for dimension 2 with size 1"
        }
    },
    {
        "name": "pytorch_CycleGAN_and_pix2pix",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model does not support tuning batch size"
        }
    },
    {
        "name": "pytorch_stargan",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "NotImplemented",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model doesn't support customizing batch size."
        }
    },
    {
        "name": "pytorch_struct",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "ValueError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "max() arg is an empty sequence"
        }
    },
    {
        "name": "pytorch_unet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 69.2146165,
                    "tflops": 14.055758084121763
                },
                {
                    "batch_size": 2,
                    "latency_ms": 64.75322700000001,
                    "tflops": 14.963793319654949
                },
                {
                    "batch_size": 4,
                    "latency_ms": 61.624035,
                    "tflops": 15.769095232399362
                },
                {
                    "batch_size": 8,
                    "latency_ms": 60.5505818125,
                    "tflops": 16.119490281015384
                },
                {
                    "batch_size": 16,
                    "latency_ms": 59.9551210625,
                    "tflops": 16.246650703833556
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 4.68 GiB (GPU 0; 39.59 GiB total capacity; 29.10 GiB already allocated; 3.70 GiB free; 33.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 16,
            "optimal_tflops_bs": 16
        }
    },
    {
        "name": "resnet18",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 3.9841995,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.070327,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.045491625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.5472561250000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.33202059375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.24125128125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.2065140390625,
                    "tflops": 9.95166564390458
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.1961881640625,
                    "tflops": 10.71051257518722
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "resnet50",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 10.597335000000001,
                    "tflops": 0.9535946355156644
                },
                {
                    "batch_size": 2,
                    "latency_ms": 5.052765,
                    "tflops": 1.930521979861375
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.521452375,
                    "tflops": 3.547530586919076
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.4393919375,
                    "tflops": 6.067752422005504
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.9344841562499999,
                    "tflops": 8.961969187094983
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.772174125,
                    "tflops": 10.440386986327614
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.71313625,
                    "tflops": 11.424893598941615
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.6848472890624999,
                    "tflops": 11.862522578226478
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "resnet50_quantized_qat",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "NotImplemented",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "The eval test only supports CPU."
        }
    },
    {
        "name": "resnext50_32x4d",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 10.423962,
                    "tflops": 1.1769969375229323
                },
                {
                    "batch_size": 2,
                    "latency_ms": 5.547208,
                    "tflops": 2.310134612794416
                },
                {
                    "batch_size": 4,
                    "latency_ms": 3.357343875,
                    "tflops": 3.430225278857342
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.715212125,
                    "tflops": 7.214598339630204
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.3149245,
                    "tflops": 9.536223008327434
                },
                {
                    "batch_size": 32,
                    "latency_ms": 1.113431390625,
                    "tflops": 11.355305824810586
                },
                {
                    "batch_size": 64,
                    "latency_ms": 1.0308243046875,
                    "tflops": 12.294787215293118
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.98073098046875,
                    "tflops": 12.89166636677204
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "shufflenet_v2_x1_0",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 9.3701765,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 4.49208375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.4437274999999996,
                    "tflops": 0.1905794408532591
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.185289125,
                    "tflops": 0.3622840248509128
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.6973615625,
                    "tflops": 0.6208360880082193
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.29417103125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.1586856484375,
                    "tflops": 2.7184519822011546
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.097443109375,
                    "tflops": 4.584359094977964
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "soft_actor_critic",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "NotImplemented",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [],
            "error_message": "Model doesn't support customizing batch size."
        }
    },
    {
        "name": "speech_transformer",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 5695.497254,
                    "tflops": 0.5059956767793162
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2852.1103935,
                    "tflops": 0.5023986174833494
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1430.2795150000002,
                    "tflops": 0.5015051527993063
                },
                {
                    "batch_size": 8,
                    "latency_ms": 719.9748460625001,
                    "tflops": 0.498505265697821
                },
                {
                    "batch_size": 16,
                    "latency_ms": 356.776523375,
                    "tflops": 0.500792487637542
                },
                {
                    "batch_size": 32,
                    "latency_ms": 178.358287125,
                    "tflops": 0.5020494174433492
                },
                {
                    "batch_size": 64,
                    "latency_ms": 89.5476804140625,
                    "tflops": 0.5011390477558479
                },
                {
                    "batch_size": 128,
                    "latency_ms": 44.783520109375004,
                    "tflops": 0.5004232527082192
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 1
        }
    },
    {
        "name": "squeezenet1_1",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 3.611147,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.0254795000000003,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 0.991097,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.5075585625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.25583021875,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.15922,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.1236514765625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.111919796875,
                    "tflops": 6.285966898055984
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "tacotron2",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 284.38848800000005,
                    "tflops": 0.06863052892480473
                },
                {
                    "batch_size": 2,
                    "latency_ms": 436.14787425,
                    "tflops": 0.1427547709893309
                },
                {
                    "batch_size": 4,
                    "latency_ms": 223.164527125,
                    "tflops": 0.27775558919806115
                },
                {
                    "batch_size": 8,
                    "latency_ms": 137.95045881250002,
                    "tflops": 1.263518754432717
                },
                {
                    "batch_size": 16,
                    "latency_ms": 70.7089226875,
                    "tflops": 1.360847995258051
                },
                {
                    "batch_size": 32,
                    "latency_ms": 36.132441,
                    "tflops": 1.5131707883304701
                },
                {
                    "batch_size": 64,
                    "latency_ms": 18.6929446796875,
                    "tflops": 2.6877614521814497
                },
                {
                    "batch_size": 128,
                    "latency_ms": 9.74041246875,
                    "tflops": 5.203095413175837
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "timm_efficientdet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 76.8008095,
                    "tflops": 0.3085332969537443
                },
                {
                    "batch_size": 2,
                    "latency_ms": 47.00558375,
                    "tflops": 0.47916097279592174
                },
                {
                    "batch_size": 4,
                    "latency_ms": 33.022428875,
                    "tflops": 0.7196845324790717
                },
                {
                    "batch_size": 8,
                    "latency_ms": 26.10964375,
                    "tflops": 0.934776197373274
                },
                {
                    "batch_size": 16,
                    "latency_ms": 24.661675187500002,
                    "tflops": 0.9512297134936667
                },
                {
                    "batch_size": 32,
                    "latency_ms": 23.28206634375,
                    "tflops": 1.025603778268123
                },
                {
                    "batch_size": 64,
                    "latency_ms": 19.843853703125,
                    "tflops": 1.2295178000819618
                },
                {
                    "batch_size": 128,
                    "latency_ms": 11.83725552734375,
                    "tflops": 1.6326034359088968
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "timm_efficientnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 10.3285605,
                    "tflops": 0.15532238203984655
                },
                {
                    "batch_size": 2,
                    "latency_ms": 5.4046330000000005,
                    "tflops": 0.2866230415436661
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.8018535,
                    "tflops": 0.5264668676091511
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.38097525,
                    "tflops": 0.9879796375165357
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.6980073437500001,
                    "tflops": 1.8951899155019198
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.384824671875,
                    "tflops": 3.6059200033939947
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.3151816796875,
                    "tflops": 4.683860878569976
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.28615450781249996,
                    "tflops": 5.233029459122095
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "timm_nfnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 20.421831,
                    "tflops": 2.776474530649051
                },
                {
                    "batch_size": 2,
                    "latency_ms": 10.096180749999998,
                    "tflops": 3.3409053760943004
                },
                {
                    "batch_size": 4,
                    "latency_ms": 6.061676,
                    "tflops": 3.8855089772447027
                },
                {
                    "batch_size": 8,
                    "latency_ms": 3.28926225,
                    "tflops": 5.433160906618832
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.8066353125,
                    "tflops": 8.068732191787555
                },
                {
                    "batch_size": 32,
                    "latency_ms": 1.40499284375,
                    "tflops": 9.719530656235198
                },
                {
                    "batch_size": 64,
                    "latency_ms": 1.217842875,
                    "tflops": 10.951855571946092
                },
                {
                    "batch_size": 128,
                    "latency_ms": 1.12180797265625,
                    "tflops": 11.803961412071889
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "timm_regnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 17.947952,
                    "tflops": 1.7678674390318732
                },
                {
                    "batch_size": 2,
                    "latency_ms": 9.412302,
                    "tflops": 3.1310550604385643
                },
                {
                    "batch_size": 4,
                    "latency_ms": 5.680462,
                    "tflops": 5.152409319460593
                },
                {
                    "batch_size": 8,
                    "latency_ms": 4.568542875,
                    "tflops": 6.5850084020394295
                },
                {
                    "batch_size": 16,
                    "latency_ms": 3.22640321875,
                    "tflops": 8.729886349385707
                },
                {
                    "batch_size": 32,
                    "latency_ms": 2.472434296875,
                    "tflops": 11.334613593224619
                },
                {
                    "batch_size": 64,
                    "latency_ms": 3.4985664609375,
                    "tflops": 8.025890453151494
                },
                {
                    "batch_size": 128,
                    "latency_ms": 3.8259746874999996,
                    "tflops": 6.947364225723994
                }
            ],
            "optimal_latency_bs": 32,
            "optimal_tflops_bs": 32
        }
    },
    {
        "name": "timm_resnest",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 5.9416545,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.72834125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.7409267499999999,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.0593114375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.78965425,
                    "tflops": 5.50438331342199
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.4678130625,
                    "tflops": 7.194619690878301
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.429500484375,
                    "tflops": 9.571162277963186
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.41414731640625,
                    "tflops": 10.006384583045767
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "timm_vision_transformer",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 8.5519865,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 4.80270325,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 2.3562797499999997,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.3155671875000001,
                    "tflops": 10.001186141041874
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.06407315625,
                    "tflops": 12.432823637575115
                },
                {
                    "batch_size": 32,
                    "latency_ms": 1.002625828125,
                    "tflops": 13.321728049065326
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.9229548046875,
                    "tflops": 14.550101119728712
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.88419573046875,
                    "tflops": 15.183976308462729
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "timm_vovnet",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 7.699336,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 3.922931,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.853457125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.2310610625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.9517113749999999,
                    "tflops": 10.719209940182264
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.849647671875,
                    "tflops": 12.115168869569937
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.8228958515625,
                    "tflops": 12.909144753957161
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.8569859570312499,
                    "tflops": 13.74823962696548
                }
            ],
            "optimal_latency_bs": 64,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "tts_angular",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 4.546177500000001,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.274325,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.17136125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 0.75674375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 16,
                    "latency_ms": 0.4155330625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 32,
                    "latency_ms": 0.217928703125,
                    "tflops": 0.0
                },
                {
                    "batch_size": 64,
                    "latency_ms": 0.1529432890625,
                    "tflops": 0.0
                },
                {
                    "batch_size": 128,
                    "latency_ms": 0.11093665234375,
                    "tflops": 9.431761456292897
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 128
        }
    },
    {
        "name": "vgg16",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "OK",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 3.5456494999999997,
                    "tflops": 0.0
                },
                {
                    "batch_size": 2,
                    "latency_ms": 2.39077575,
                    "tflops": 0.0
                },
                {
                    "batch_size": 4,
                    "latency_ms": 1.993235375,
                    "tflops": 0.0
                },
                {
                    "batch_size": 8,
                    "latency_ms": 1.6809761875,
                    "tflops": 12.402828611073817
                },
                {
                    "batch_size": 16,
                    "latency_ms": 1.17800778125,
                    "tflops": 11.885826288358421
                },
                {
                    "batch_size": 32,
                    "latency_ms": 1.068707265625,
                    "tflops": 11.630693651926716
                },
                {
                    "batch_size": 64,
                    "latency_ms": 1.0235972890625,
                    "tflops": 12.058740455883127
                },
                {
                    "batch_size": 128,
                    "latency_ms": 1.0015835,
                    "tflops": 12.316857829693062
                }
            ],
            "optimal_latency_bs": 128,
            "optimal_tflops_bs": 8
        }
    },
    {
        "name": "vision_maskrcnn",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 64.365558,
                    "tflops": 6.135582848967245
                },
                {
                    "batch_size": 2,
                    "latency_ms": 61.41848625,
                    "tflops": 6.625857250339579
                },
                {
                    "batch_size": 4,
                    "latency_ms": 77.423586,
                    "tflops": 9.106628334032662
                },
                {
                    "batch_size": 8,
                    "latency_ms": 66.99113750000001,
                    "tflops": 8.176154140435585
                },
                {
                    "batch_size": 16,
                    "latency_ms": 59.1388240625,
                    "tflops": 8.504828200518652
                },
                {
                    "batch_size": 32,
                    "latency_ms": 64.400492171875,
                    "tflops": 8.413057464356836
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 6.89 GiB (GPU 0; 39.59 GiB total capacity; 27.51 GiB already allocated; 335.94 MiB free; 36.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 16,
            "optimal_tflops_bs": 4
        }
    },
    {
        "name": "yolov3",
        "test": "eval",
        "device": "cuda",
        "extra_args": [
            "--precision",
            "fp32"
        ],
        "status": "RuntimeError",
        "batch_size": null,
        "precision": "fp32",
        "results": {
            "details": [
                {
                    "batch_size": 1,
                    "latency_ms": 15.1306695,
                    "tflops": 3.58791156985822
                },
                {
                    "batch_size": 2,
                    "latency_ms": 7.778909499999999,
                    "tflops": 7.283689589073502
                },
                {
                    "batch_size": 4,
                    "latency_ms": 5.930875875,
                    "tflops": 10.164252192473688
                },
                {
                    "batch_size": 8,
                    "latency_ms": 5.2385725,
                    "tflops": 11.918623007258013
                },
                {
                    "batch_size": 16,
                    "latency_ms": 4.01784896875,
                    "tflops": 11.0209249508419
                },
                {
                    "batch_size": 32,
                    "latency_ms": 3.7250435625,
                    "tflops": 11.548930143135182
                },
                {
                    "batch_size": 64,
                    "latency_ms": 3.61219565625,
                    "tflops": 12.016877030361691
                }
            ],
            "error_message": "CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 39.59 GiB total capacity; 35.95 GiB already allocated; 153.94 MiB free; 37.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
            "optimal_latency_bs": 64,
            "optimal_tflops_bs": 64
        }
    }
]